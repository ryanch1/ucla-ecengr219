{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Problem Setup/Definition:\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction import text, stop_words\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.svm import LinearSVC\n",
    "import math \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
      "[2 1 2 ... 2 3 4]\n",
      "First 10 articles Classification (H&F): \n",
      "[0, 0, 0, 0, 1, 1, 0, 1, 0, 1]\n",
      "First 10 articles Classification (NO H&F): \n",
      "[1, 1, 0, 1, 1, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "def my_custom_preprocessor(doc_string):\n",
    "    # do all data preprocessing here\n",
    "    \n",
    "    # Lower case\n",
    "    doc_string=doc_string.lower()\n",
    "    \n",
    "    # Remove Numbers\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    doc_string.translate(remove_digits)\n",
    "    \n",
    "    # Convert to tokenized form....\n",
    "    tokens = nltk.tokenize.word_tokenize(doc_string)\n",
    "    # Iterate through list of tokens (words) and remove all numbers\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Iterate through list of tokens (words) and stem (shorten) each word\n",
    "    port_stemmer = PorterStemmer()\n",
    "    tokens = [port_stemmer.stem(words) for words in tokens ]\n",
    "    # Iterate through list of tokens (words) and remove all stopwords\n",
    "    tokens_no_stop = []\n",
    "    stop_words = text.ENGLISH_STOP_WORDS\n",
    "    for words in tokens:\n",
    "        if not words in stop_words:\n",
    "            tokens_no_stop.append(words)\n",
    "    \n",
    "    ###############################\n",
    "    #### Lemmatize with pos_tag ###\n",
    "    ###############################\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Convert between two different tagging schemes\n",
    "    def change_tags(penntag):\n",
    "        morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                      'VB':'v', 'RB':'r'}\n",
    "        try:\n",
    "            return morphy_tag[penntag[:2]]\n",
    "        except:\n",
    "            return 'n'\n",
    "        \n",
    "    tokens_no_stop = [lemmatizer.lemmatize(word.lower(), pos=change_tags(tag)) for word, tag in pos_tag(tokens_no_stop)]\n",
    "    \n",
    "    # Rejoin List of tokens and return that single document-string\n",
    "    return ' '.join(tokens_no_stop)\n",
    "\n",
    "###########################\n",
    "#### RoC Curve Function ###\n",
    "###########################\n",
    "\n",
    "def plot_roc(fpr, tpr):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    ax.plot(fpr, tpr, lw=2, label= 'area under curve = %0.4f' % roc_auc)\n",
    "\n",
    "    ax.grid(color='0.7', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate',fontsize=15)\n",
    "    ax.set_ylabel('True Positive Rate',fontsize=15)\n",
    "\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    for label in ax.get_xticklabels()+ax.get_yticklabels():\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "def fit_predict_and_plot_roc(pipe, train_data, train_label, test_data, test_label):\n",
    "    pipe.fit(train_data, train_label)\n",
    "\n",
    "    if hasattr(pipe, 'decision_function'):\n",
    "        prob_score = pipe.decision_function(test_data)\n",
    "        fpr, tpr, _ = roc_curve(test_label, prob_score)\n",
    "    else:\n",
    "        prob_score = pipe.predict_proba(test_data)\n",
    "        fpr, tpr, _ = roc_curve(test_label, prob_score[:,1])\n",
    "\n",
    "    plot_roc(fpr, tpr)\n",
    "    \n",
    "#####################################################\n",
    "#### Define Custom stop words for CountVectorizer ###\n",
    "#####################################################\n",
    "\n",
    "stop_words_skt = text.ENGLISH_STOP_WORDS\n",
    "stop_words_en = stopwords.words('english')\n",
    "combined_stopwords = set.union(set(stop_words_en),set(punctuation),set(stop_words_skt))\n",
    "\n",
    "# Run stop_words through the same pre-processor as the document-matrix\n",
    "# This will apply stemmed/lemmatized stop_woirds to stemmed/lemmatized tokenized document lists\n",
    "def process_stop_words(stop_word_set):\n",
    "    doc_string = ' '.join(stop_word_set)\n",
    "    return my_custom_preprocessor(doc_string).split()\n",
    "\n",
    "################################\n",
    "#### Estimator Helper Class  ###\n",
    "################################\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]\n",
    "\n",
    "##################################\n",
    "#### Import Dataset Train/Test ###\n",
    "##################################\n",
    "\n",
    "# Only take a specific selection (8) of the 20 available categories\n",
    "categories = ['comp.graphics', 'comp.os.ms-windows.misc',\n",
    "'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "'rec.autos', 'rec.motorcycles',\n",
    "'rec.sport.baseball', 'rec.sport.hockey']\n",
    "\n",
    "# Load a full data sets consisting of those 8 categories, one with headers + footers, one without\n",
    "# Dont need \"test\" set with cross validation; splits up entire set for you k-fold times\n",
    "dataset = fetch_20newsgroups(subset = 'all', categories = categories, shuffle = True, random_state = None)\n",
    "\n",
    "## Load training & test data sets WITHOUT headers & footers\n",
    "dataset_no_hf = fetch_20newsgroups(subset = 'all', categories = categories, shuffle = True, random_state = None, remove=['headers', 'footers'])\n",
    "\n",
    "# Clean the data sets before analysis: \n",
    "cleaned_dataset = []\n",
    "for documents in range(len(dataset.data)):\n",
    "    cleaned_dataset.append(my_custom_preprocessor(dataset.data[documents]))\n",
    "    \n",
    "cleaned_dataset_no_hf = []\n",
    "for documents in range(len(dataset_no_hf.data)):\n",
    "    cleaned_dataset_no_hf.append(my_custom_preprocessor(dataset_no_hf.data[documents]))\n",
    "\n",
    "print(\"\\n\\n\" + '-'*40 + \"\\n\\n\")\n",
    "\n",
    "#############################################\n",
    "#### Define Class data set arrys (0 or 1) ###\n",
    "#############################################\n",
    "# Categorize the 8 news categories into two (binary) Classes \n",
    "# 0 = computer technology\n",
    "# 1 = recreational activity\n",
    "data_class = [] \n",
    "data_class_no_hf = []\n",
    "\n",
    "# Categories are mapped 0-7, (0-3) = Comp, (4-7) = Recreation\n",
    "print(dataset.target_names)\n",
    "print(dataset.target)\n",
    "\n",
    "for category in dataset.target:\n",
    "    if category < 4:\n",
    "        data_class.append(0)\n",
    "    else:\n",
    "        data_class.append(1)\n",
    "        \n",
    "        \n",
    "for category in dataset_no_hf.target:\n",
    "    if category < 4:\n",
    "        data_class_no_hf.append(0)\n",
    "    else:\n",
    "        data_class_no_hf.append(1)\n",
    "        \n",
    "# Sanity Checks, values should all be either 1 or 0\n",
    "print(\"First 10 articles Classification (H&F): \\n\" + str(data_class[0:10]))\n",
    "print(\"First 10 articles Classification (NO H&F): \\n\" + str(data_class_no_hf[0:10]))\n",
    "# This will be used for the classification categories only!!!!\n",
    "# Each data point refers to the classification of a single article in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmpsl8epos8'\", use \"location='/tmp/tmpsl8epos8'\" instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "## Create Pipelines for Comparison ##\n",
    "#####################################\n",
    "#enable Cachine\n",
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=0)\n",
    "\n",
    "### Initial Pipeline ###\n",
    "# These tuples() will be altered via the 'param_grid' List[]\n",
    "pipeline_hf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('dim_reducer', TruncatedSVD()),\n",
    "    ('classifier', LinearSVC(max_iter=5000)),\n",
    "],\n",
    "memory=memory\n",
    ")\n",
    "\n",
    "pipeline_no_hf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('dim_reducer', TruncatedSVD()),\n",
    "    ('classifier', LinearSVC(max_iter=5000)),\n",
    "],\n",
    "memory=memory\n",
    ")\n",
    "\n",
    "######################\n",
    "## Cross Validation ##\n",
    "######################\n",
    "# An List[] of Dictionary{key:value} parameters that will be iterated over\n",
    "# Each Dictionary{} in the List[] references different types of 'vectorizer', 'tfidf', etc.\n",
    "# Alter the range of hyperparameters within each Dictionary{} with <estimator>__<parameter>.\n",
    "# E.g. Try both 3 & 5 min_df values for CountVectorizer().... '<vectorizer>__<min_df>'': [3,5] \n",
    "\n",
    "# Options to Iterate Over:\n",
    "\n",
    "MIN_DIF = [3,5]\n",
    "# Lemm + Stemm or Defualt\n",
    "TOKEN_PATTERN = [r'(?u)\\b\\w\\w+\\b',r'(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b']\n",
    "REDUCER_OPTIONS = [TruncatedSVD(), NMF()]\n",
    "REDUCER_N_COMPONENTS = [50]\n",
    "# LinearSVC() values\n",
    "OPTIMAL_LINEAR_C_VALUE = [100]\n",
    "# LogisticRegression() Values\n",
    "LOG_REG_PENALTIES = ['l1', 'l2']\n",
    "OPTIMAL_LOG_REG_C_VALUE = [100]\n",
    "\n",
    "param_grid = [\n",
    "                { # Linear Classifier \n",
    "                    'vectorizer__min_df': MIN_DIF,\n",
    "                    'vectorizer__token_pattern': TOKEN_PATTERN,\n",
    "                    'dim_reducer': REDUCER_OPTIONS,\n",
    "                    'dim_reducer__n_components': REDUCER_N_COMPONENTS,\n",
    "                    'classifier': [LinearSVC()],\n",
    "                    'classifier__C':OPTIMAL_LINEAR_C_VALUE\n",
    "                },\n",
    "    \n",
    "                { # Logisitc Regresion\n",
    "                    'vectorizer__min_df': MIN_DIF,\n",
    "                    'vectorizer__token_pattern': TOKEN_PATTERN,\n",
    "                    'dim_reducer': REDUCER_OPTIONS,\n",
    "                    'dim_reducer__n_components': REDUCER_N_COMPONENTS,\n",
    "                    'classifier': [LogisticRegression(solver='liblinear', max_iter=5000)],\n",
    "                    'classifier__penalty': LOG_REG_PENALTIES,\n",
    "                    'classifier__C':OPTIMAL_LOG_REG_C_VALUE                        \n",
    "                },\n",
    "    \n",
    "                { # Naive Bayes Gaussian\n",
    "                    'vectorizer__min_df': MIN_DIF,\n",
    "                    'vectorizer__token_pattern': TOKEN_PATTERN,\n",
    "                    'dim_reducer': REDUCER_OPTIONS,\n",
    "                    'dim_reducer__n_components': REDUCER_N_COMPONENTS,\n",
    "                    'classifier': [GaussianNB()],                    \n",
    "                }    \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.10s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.99s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.08s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.10s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.10s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.48s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.61s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.62s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.51s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.61s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.53s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.42s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.43s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.54s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.53s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.40s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=Memory(location=/tmp/tmpsl8epos8/joblib),\n",
       "                                steps=[('vectorizer',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=N...\n",
       "                                              init=None, l1_ratio=0.0,\n",
       "                                              max_iter=200, n_components=None,\n",
       "                                              random_state=None, shuffle=False,\n",
       "                                              solver='cd', tol=0.0001,\n",
       "                                              verbose=0)],\n",
       "                          'dim_reducer__n_components': [50],\n",
       "                          'vectorizer__min_df': [3, 5],\n",
       "                          'vectorizer__token_pattern': ['(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                        '(?u)\\\\b[^\\\\W\\\\d_][^\\\\W\\\\d_][^\\\\W\\\\d_]+\\\\b']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cross Validate/iterate over pipeline; data has header/footer included\n",
    "grid = GridSearchCV(pipeline_hf, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid.fit(cleaned_dataset, dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.86s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.85s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.78s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.83s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.88s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.76s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.75s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.81s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.76s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.35s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.37s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.35s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.31s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.09s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    }
   ],
   "source": [
    "## Cross Validate/iterate over pipeline; data has header/footer removed\n",
    "grid_no_hf = GridSearchCV(pipeline_no_hf, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid_no_hf.fit(cleaned_dataset_no_hf, dataset_no_hf.target)\n",
    "\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_dim_reducer</th>\n",
       "      <th>param_dim_reducer__n_components</th>\n",
       "      <th>param_vectorizer__min_df</th>\n",
       "      <th>param_vectorizer__token_pattern</th>\n",
       "      <th>...</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>Has Header + Footer</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.945943</td>\n",
       "      <td>0.017245</td>\n",
       "      <td>0.178805</td>\n",
       "      <td>0.005491</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.870640</td>\n",
       "      <td>0.880786</td>\n",
       "      <td>0.869289</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.861675</td>\n",
       "      <td>0.869575</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.586256</td>\n",
       "      <td>0.061375</td>\n",
       "      <td>0.177286</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.872543</td>\n",
       "      <td>0.880152</td>\n",
       "      <td>0.868020</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.861675</td>\n",
       "      <td>0.869575</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.956435</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>0.158251</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.869372</td>\n",
       "      <td>0.884591</td>\n",
       "      <td>0.862310</td>\n",
       "      <td>0.864848</td>\n",
       "      <td>0.862310</td>\n",
       "      <td>0.868686</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.580110</td>\n",
       "      <td>0.096306</td>\n",
       "      <td>0.153366</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.870640</td>\n",
       "      <td>0.884591</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.862310</td>\n",
       "      <td>0.868686</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.784404</td>\n",
       "      <td>0.043562</td>\n",
       "      <td>0.173576</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.873811</td>\n",
       "      <td>0.877616</td>\n",
       "      <td>0.869924</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.855330</td>\n",
       "      <td>0.868433</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.605547</td>\n",
       "      <td>0.070769</td>\n",
       "      <td>0.154724</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.871275</td>\n",
       "      <td>0.871275</td>\n",
       "      <td>0.861041</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.871827</td>\n",
       "      <td>0.868180</td>\n",
       "      <td>0.004259</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.988868</td>\n",
       "      <td>0.061038</td>\n",
       "      <td>0.158344</td>\n",
       "      <td>0.005003</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.869372</td>\n",
       "      <td>0.885859</td>\n",
       "      <td>0.864848</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.857868</td>\n",
       "      <td>0.868178</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.899801</td>\n",
       "      <td>0.013383</td>\n",
       "      <td>0.156915</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.869372</td>\n",
       "      <td>0.871909</td>\n",
       "      <td>0.863579</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.872462</td>\n",
       "      <td>0.868053</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.151419</td>\n",
       "      <td>0.114914</td>\n",
       "      <td>0.155776</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.870006</td>\n",
       "      <td>0.871909</td>\n",
       "      <td>0.861675</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.869924</td>\n",
       "      <td>0.867799</td>\n",
       "      <td>0.003719</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.486450</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0.175620</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.866836</td>\n",
       "      <td>0.880152</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>0.866276</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.880617</td>\n",
       "      <td>0.005652</td>\n",
       "      <td>0.175116</td>\n",
       "      <td>0.003485</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.871275</td>\n",
       "      <td>0.876982</td>\n",
       "      <td>0.864848</td>\n",
       "      <td>0.864848</td>\n",
       "      <td>0.853426</td>\n",
       "      <td>0.866276</td>\n",
       "      <td>0.007859</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.953710</td>\n",
       "      <td>0.059121</td>\n",
       "      <td>0.176137</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.866836</td>\n",
       "      <td>0.876982</td>\n",
       "      <td>0.862310</td>\n",
       "      <td>0.860406</td>\n",
       "      <td>0.850254</td>\n",
       "      <td>0.863357</td>\n",
       "      <td>0.008709</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.713476</td>\n",
       "      <td>0.037696</td>\n",
       "      <td>0.265653</td>\n",
       "      <td>0.004616</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.845276</td>\n",
       "      <td>0.829315</td>\n",
       "      <td>0.830584</td>\n",
       "      <td>0.831218</td>\n",
       "      <td>0.835953</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.677738</td>\n",
       "      <td>0.028749</td>\n",
       "      <td>0.279799</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.849081</td>\n",
       "      <td>0.824239</td>\n",
       "      <td>0.831218</td>\n",
       "      <td>0.825508</td>\n",
       "      <td>0.834684</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.914340</td>\n",
       "      <td>3.588560</td>\n",
       "      <td>0.263774</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.840837</td>\n",
       "      <td>0.845910</td>\n",
       "      <td>0.825508</td>\n",
       "      <td>0.827411</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>0.833923</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.553790</td>\n",
       "      <td>0.008520</td>\n",
       "      <td>0.265980</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.841471</td>\n",
       "      <td>0.824239</td>\n",
       "      <td>0.831853</td>\n",
       "      <td>0.824239</td>\n",
       "      <td>0.832781</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.709743</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.280085</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.830584</td>\n",
       "      <td>0.826777</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>True</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.695342</td>\n",
       "      <td>0.022746</td>\n",
       "      <td>0.292930</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.836398</td>\n",
       "      <td>0.847178</td>\n",
       "      <td>0.824873</td>\n",
       "      <td>0.824873</td>\n",
       "      <td>0.826142</td>\n",
       "      <td>0.831893</td>\n",
       "      <td>0.008781</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.440981</td>\n",
       "      <td>1.837307</td>\n",
       "      <td>0.281934</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.833862</td>\n",
       "      <td>0.844008</td>\n",
       "      <td>0.825508</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>0.822335</td>\n",
       "      <td>0.831132</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21.318332</td>\n",
       "      <td>4.180452</td>\n",
       "      <td>0.281615</td>\n",
       "      <td>0.006133</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.838301</td>\n",
       "      <td>0.845910</td>\n",
       "      <td>0.820431</td>\n",
       "      <td>0.829315</td>\n",
       "      <td>0.820431</td>\n",
       "      <td>0.830878</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.541912</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.283078</td>\n",
       "      <td>0.005985</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.833228</td>\n",
       "      <td>0.847812</td>\n",
       "      <td>0.821701</td>\n",
       "      <td>0.831218</td>\n",
       "      <td>0.807741</td>\n",
       "      <td>0.828340</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>True</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.604376</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>0.279256</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.828155</td>\n",
       "      <td>0.841471</td>\n",
       "      <td>0.823604</td>\n",
       "      <td>0.824873</td>\n",
       "      <td>0.813452</td>\n",
       "      <td>0.826311</td>\n",
       "      <td>0.009033</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.906259</td>\n",
       "      <td>3.520158</td>\n",
       "      <td>0.300709</td>\n",
       "      <td>0.006172</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.832594</td>\n",
       "      <td>0.838301</td>\n",
       "      <td>0.819162</td>\n",
       "      <td>0.816624</td>\n",
       "      <td>0.821701</td>\n",
       "      <td>0.825676</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>True</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.589196</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>0.293664</td>\n",
       "      <td>0.005328</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.830057</td>\n",
       "      <td>0.835130</td>\n",
       "      <td>0.828046</td>\n",
       "      <td>0.817259</td>\n",
       "      <td>0.810914</td>\n",
       "      <td>0.824281</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.299105</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>0.179782</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.748890</td>\n",
       "      <td>0.723526</td>\n",
       "      <td>0.736675</td>\n",
       "      <td>0.748731</td>\n",
       "      <td>0.756980</td>\n",
       "      <td>0.742960</td>\n",
       "      <td>0.011681</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.309508</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.159681</td>\n",
       "      <td>0.004257</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.739379</td>\n",
       "      <td>0.721623</td>\n",
       "      <td>0.739213</td>\n",
       "      <td>0.741751</td>\n",
       "      <td>0.759518</td>\n",
       "      <td>0.740297</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.347842</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.181297</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.737476</td>\n",
       "      <td>0.716550</td>\n",
       "      <td>0.729695</td>\n",
       "      <td>0.735406</td>\n",
       "      <td>0.743655</td>\n",
       "      <td>0.732557</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.360400</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>0.163471</td>\n",
       "      <td>0.004992</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.752061</td>\n",
       "      <td>0.722257</td>\n",
       "      <td>0.720178</td>\n",
       "      <td>0.734772</td>\n",
       "      <td>0.717640</td>\n",
       "      <td>0.729381</td>\n",
       "      <td>0.012781</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.361188</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.283016</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.714648</td>\n",
       "      <td>0.726696</td>\n",
       "      <td>0.739213</td>\n",
       "      <td>0.725254</td>\n",
       "      <td>0.725254</td>\n",
       "      <td>0.726213</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.313148</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.269601</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.733037</td>\n",
       "      <td>0.718453</td>\n",
       "      <td>0.720178</td>\n",
       "      <td>0.718274</td>\n",
       "      <td>0.725254</td>\n",
       "      <td>0.723039</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.300665</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.288486</td>\n",
       "      <td>0.006765</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.714014</td>\n",
       "      <td>0.720989</td>\n",
       "      <td>0.724619</td>\n",
       "      <td>0.713198</td>\n",
       "      <td>0.735406</td>\n",
       "      <td>0.721645</td>\n",
       "      <td>0.008102</td>\n",
       "      <td>True</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.348107</td>\n",
       "      <td>0.004079</td>\n",
       "      <td>0.298987</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.703234</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.718909</td>\n",
       "      <td>0.708756</td>\n",
       "      <td>0.730330</td>\n",
       "      <td>0.719234</td>\n",
       "      <td>0.012130</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "17       0.945943      0.017245         0.178805        0.005491   \n",
       "9        1.586256      0.061375         0.177286        0.005182   \n",
       "16       0.956435      0.013438         0.158251        0.003820   \n",
       "8        1.580110      0.096306         0.153366        0.003977   \n",
       "1        7.784404      0.043562         0.173576        0.004308   \n",
       "10       1.605547      0.070769         0.154724        0.003794   \n",
       "0        7.988868      0.061038         0.158344        0.005003   \n",
       "18       0.899801      0.013383         0.156915        0.003560   \n",
       "2        7.151419      0.114914         0.155776        0.004122   \n",
       "11       1.486450      0.044939         0.175620        0.003588   \n",
       "19       0.880617      0.005652         0.175116        0.003485   \n",
       "3        6.953710      0.059121         0.176137        0.003815   \n",
       "14       0.713476      0.037696         0.265653        0.004616   \n",
       "15       0.677738      0.028749         0.279799        0.004503   \n",
       "6       21.914340      3.588560         0.263774        0.005039   \n",
       "22       0.553790      0.008520         0.265980        0.005945   \n",
       "12       0.709743      0.023623         0.280085        0.005012   \n",
       "13       0.695342      0.022746         0.292930        0.005987   \n",
       "4       24.440981      1.837307         0.281934        0.007564   \n",
       "7       21.318332      4.180452         0.281615        0.006133   \n",
       "23       0.541912      0.005400         0.283078        0.005985   \n",
       "20       0.604376      0.007166         0.279256        0.003726   \n",
       "5       24.906259      3.520158         0.300709        0.006172   \n",
       "21       0.589196      0.008611         0.293664        0.005328   \n",
       "27       0.299105      0.003484         0.179782        0.003195   \n",
       "26       0.309508      0.005454         0.159681        0.004257   \n",
       "25       0.347842      0.004227         0.181297        0.004353   \n",
       "24       0.360400      0.003261         0.163471        0.004992   \n",
       "28       0.361188      0.006727         0.283016        0.005811   \n",
       "30       0.313148      0.004645         0.269601        0.006071   \n",
       "31       0.300665      0.003788         0.288486        0.006765   \n",
       "29       0.348107      0.004079         0.298987        0.005472   \n",
       "\n",
       "                                     param_classifier param_classifier__C  \\\n",
       "17  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "9   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "16  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "8   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "1   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "10  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "0   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "18  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "2   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "11  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "19  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "3   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "14  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "15  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "6   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "22  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "12  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "13  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "4   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "7   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "23  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "20  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "5   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "21  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "27       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "26       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "25       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "24       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "28       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "30       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "31       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "29       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "\n",
       "                                    param_dim_reducer  \\\n",
       "17  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "9   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "16  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "8   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "1   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "10  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "0   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "18  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "2   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "11  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "19  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "3   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "14  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "15  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "6   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "22  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "12  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "13  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "4   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "7   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "23  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "20  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "5   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "21  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "27  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "26  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "25  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "24  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "28  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "30  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "31  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "29  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "\n",
       "   param_dim_reducer__n_components param_vectorizer__min_df  \\\n",
       "17                              50                        3   \n",
       "9                               50                        3   \n",
       "16                              50                        3   \n",
       "8                               50                        3   \n",
       "1                               50                        3   \n",
       "10                              50                        5   \n",
       "0                               50                        3   \n",
       "18                              50                        5   \n",
       "2                               50                        5   \n",
       "11                              50                        5   \n",
       "19                              50                        5   \n",
       "3                               50                        5   \n",
       "14                              50                        5   \n",
       "15                              50                        5   \n",
       "6                               50                        5   \n",
       "22                              50                        5   \n",
       "12                              50                        3   \n",
       "13                              50                        3   \n",
       "4                               50                        3   \n",
       "7                               50                        5   \n",
       "23                              50                        5   \n",
       "20                              50                        3   \n",
       "5                               50                        3   \n",
       "21                              50                        3   \n",
       "27                              50                        5   \n",
       "26                              50                        5   \n",
       "25                              50                        3   \n",
       "24                              50                        3   \n",
       "28                              50                        3   \n",
       "30                              50                        5   \n",
       "31                              50                        5   \n",
       "29                              50                        3   \n",
       "\n",
       "      param_vectorizer__token_pattern  ...  \\\n",
       "17  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "9   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "16                      (?u)\\b\\w\\w+\\b  ...   \n",
       "8                       (?u)\\b\\w\\w+\\b  ...   \n",
       "1   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "10                      (?u)\\b\\w\\w+\\b  ...   \n",
       "0                       (?u)\\b\\w\\w+\\b  ...   \n",
       "18                      (?u)\\b\\w\\w+\\b  ...   \n",
       "2                       (?u)\\b\\w\\w+\\b  ...   \n",
       "11  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "19  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "3   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "14                      (?u)\\b\\w\\w+\\b  ...   \n",
       "15  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "6                       (?u)\\b\\w\\w+\\b  ...   \n",
       "22                      (?u)\\b\\w\\w+\\b  ...   \n",
       "12                      (?u)\\b\\w\\w+\\b  ...   \n",
       "13  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "4                       (?u)\\b\\w\\w+\\b  ...   \n",
       "7   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "23  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "20                      (?u)\\b\\w\\w+\\b  ...   \n",
       "5   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "21  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "27  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "26                      (?u)\\b\\w\\w+\\b  ...   \n",
       "25  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "24                      (?u)\\b\\w\\w+\\b  ...   \n",
       "28                      (?u)\\b\\w\\w+\\b  ...   \n",
       "30                      (?u)\\b\\w\\w+\\b  ...   \n",
       "31  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "29  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "\n",
       "                                               params split0_test_score  \\\n",
       "17  {'classifier': LogisticRegression(C=100, class...          0.870640   \n",
       "9   {'classifier': LogisticRegression(C=100, class...          0.872543   \n",
       "16  {'classifier': LogisticRegression(C=100, class...          0.869372   \n",
       "8   {'classifier': LogisticRegression(C=100, class...          0.870640   \n",
       "1   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.873811   \n",
       "10  {'classifier': LogisticRegression(C=100, class...          0.871275   \n",
       "0   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.869372   \n",
       "18  {'classifier': LogisticRegression(C=100, class...          0.869372   \n",
       "2   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.870006   \n",
       "11  {'classifier': LogisticRegression(C=100, class...          0.866836   \n",
       "19  {'classifier': LogisticRegression(C=100, class...          0.871275   \n",
       "3   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.866836   \n",
       "14  {'classifier': LogisticRegression(C=100, class...          0.843373   \n",
       "15  {'classifier': LogisticRegression(C=100, class...          0.843373   \n",
       "6   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.840837   \n",
       "22  {'classifier': LogisticRegression(C=100, class...          0.842105   \n",
       "12  {'classifier': LogisticRegression(C=100, class...          0.831325   \n",
       "13  {'classifier': LogisticRegression(C=100, class...          0.836398   \n",
       "4   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.833862   \n",
       "7   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.838301   \n",
       "23  {'classifier': LogisticRegression(C=100, class...          0.833228   \n",
       "20  {'classifier': LogisticRegression(C=100, class...          0.828155   \n",
       "5   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.832594   \n",
       "21  {'classifier': LogisticRegression(C=100, class...          0.830057   \n",
       "27  {'classifier': GaussianNB(priors=None, var_smo...          0.748890   \n",
       "26  {'classifier': GaussianNB(priors=None, var_smo...          0.739379   \n",
       "25  {'classifier': GaussianNB(priors=None, var_smo...          0.737476   \n",
       "24  {'classifier': GaussianNB(priors=None, var_smo...          0.752061   \n",
       "28  {'classifier': GaussianNB(priors=None, var_smo...          0.714648   \n",
       "30  {'classifier': GaussianNB(priors=None, var_smo...          0.733037   \n",
       "31  {'classifier': GaussianNB(priors=None, var_smo...          0.714014   \n",
       "29  {'classifier': GaussianNB(priors=None, var_smo...          0.703234   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "17           0.880786           0.869289           0.865482   \n",
       "9            0.880152           0.868020           0.865482   \n",
       "16           0.884591           0.862310           0.864848   \n",
       "8            0.884591           0.862944           0.862944   \n",
       "1            0.877616           0.869924           0.865482   \n",
       "10           0.871275           0.861041           0.865482   \n",
       "0            0.885859           0.864848           0.862944   \n",
       "18           0.871909           0.863579           0.862944   \n",
       "2            0.871909           0.861675           0.865482   \n",
       "11           0.880152           0.865482           0.862944   \n",
       "19           0.876982           0.864848           0.864848   \n",
       "3            0.876982           0.862310           0.860406   \n",
       "14           0.845276           0.829315           0.830584   \n",
       "15           0.849081           0.824239           0.831218   \n",
       "6            0.845910           0.825508           0.827411   \n",
       "22           0.841471           0.824239           0.831853   \n",
       "12           0.843373           0.830584           0.826777   \n",
       "13           0.847178           0.824873           0.824873   \n",
       "4            0.844008           0.825508           0.829949   \n",
       "7            0.845910           0.820431           0.829315   \n",
       "23           0.847812           0.821701           0.831218   \n",
       "20           0.841471           0.823604           0.824873   \n",
       "5            0.838301           0.819162           0.816624   \n",
       "21           0.835130           0.828046           0.817259   \n",
       "27           0.723526           0.736675           0.748731   \n",
       "26           0.721623           0.739213           0.741751   \n",
       "25           0.716550           0.729695           0.735406   \n",
       "24           0.722257           0.720178           0.734772   \n",
       "28           0.726696           0.739213           0.725254   \n",
       "30           0.718453           0.720178           0.718274   \n",
       "31           0.720989           0.724619           0.713198   \n",
       "29           0.734940           0.718909           0.708756   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  Has Header + Footer  \\\n",
       "17           0.861675         0.869575        0.006420                 True   \n",
       "9            0.861675         0.869575        0.006360                 True   \n",
       "16           0.862310         0.868686        0.008360                 True   \n",
       "8            0.862310         0.868686        0.008525                 True   \n",
       "1            0.855330         0.868433        0.007692                 True   \n",
       "10           0.871827         0.868180        0.004259                 True   \n",
       "0            0.857868         0.868178        0.009579                 True   \n",
       "18           0.872462         0.868053        0.004054                 True   \n",
       "2            0.869924         0.867799        0.003719                 True   \n",
       "11           0.855964         0.866276        0.007886                 True   \n",
       "19           0.853426         0.866276        0.007859                 True   \n",
       "3            0.850254         0.863357        0.008709                 True   \n",
       "14           0.831218         0.835953        0.006889                 True   \n",
       "15           0.825508         0.834684        0.009880                 True   \n",
       "6            0.829949         0.833923        0.008006                 True   \n",
       "22           0.824239         0.832781        0.007865                 True   \n",
       "12           0.829949         0.832402        0.005701                 True   \n",
       "13           0.826142         0.831893        0.008781                 True   \n",
       "4            0.822335         0.831132        0.007532                 True   \n",
       "7            0.820431         0.830878        0.010018                 True   \n",
       "23           0.807741         0.828340        0.013265                 True   \n",
       "20           0.813452         0.826311        0.009033                 True   \n",
       "5            0.821701         0.825676        0.008335                 True   \n",
       "21           0.810914         0.824281        0.008868                 True   \n",
       "27           0.756980         0.742960        0.011681                 True   \n",
       "26           0.759518         0.740297        0.012019                 True   \n",
       "25           0.743655         0.732557        0.009164                 True   \n",
       "24           0.717640         0.729381        0.012781                 True   \n",
       "28           0.725254         0.726213        0.007808                 True   \n",
       "30           0.725254         0.723039        0.005600                 True   \n",
       "31           0.735406         0.721645        0.008102                 True   \n",
       "29           0.730330         0.719234        0.012130                 True   \n",
       "\n",
       "   rank_test_score  \n",
       "17               1  \n",
       "9                2  \n",
       "16               3  \n",
       "8                4  \n",
       "1                5  \n",
       "10               6  \n",
       "0                7  \n",
       "18               8  \n",
       "2                9  \n",
       "11              10  \n",
       "19              11  \n",
       "3               12  \n",
       "14              13  \n",
       "15              14  \n",
       "6               15  \n",
       "22              16  \n",
       "12              17  \n",
       "13              18  \n",
       "4               19  \n",
       "7               20  \n",
       "23              21  \n",
       "20              22  \n",
       "5               23  \n",
       "21              24  \n",
       "27              25  \n",
       "26              26  \n",
       "25              27  \n",
       "24              28  \n",
       "28              29  \n",
       "30              30  \n",
       "31              31  \n",
       "29              32  \n",
       "\n",
       "[32 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################\n",
    "## Grid Search Results : DATA with HEADERS + FOOTERS ##\n",
    "#######################################################\n",
    "# Add column to Table: Used data with Header and Footers INCLUDED\n",
    "table_hf = pd.DataFrame(grid.cv_results_)\n",
    "table_hf.insert (len(table_hf.columns)-1, 'Has Header + Footer', 'True')\n",
    "\n",
    "# Print and order by best 'accuracy'\n",
    "table_hf.sort_values(by=['rank_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_dim_reducer</th>\n",
       "      <th>param_dim_reducer__n_components</th>\n",
       "      <th>param_vectorizer__min_df</th>\n",
       "      <th>param_vectorizer__token_pattern</th>\n",
       "      <th>...</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>Has Header + Footer</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.894754</td>\n",
       "      <td>0.015393</td>\n",
       "      <td>0.128359</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.824350</td>\n",
       "      <td>0.816624</td>\n",
       "      <td>0.822970</td>\n",
       "      <td>0.822335</td>\n",
       "      <td>0.818702</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.204700</td>\n",
       "      <td>0.038466</td>\n",
       "      <td>0.127841</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.804692</td>\n",
       "      <td>0.824350</td>\n",
       "      <td>0.815355</td>\n",
       "      <td>0.820431</td>\n",
       "      <td>0.823604</td>\n",
       "      <td>0.817687</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.041098</td>\n",
       "      <td>0.136050</td>\n",
       "      <td>0.124596</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.804692</td>\n",
       "      <td>0.819911</td>\n",
       "      <td>0.812183</td>\n",
       "      <td>0.819797</td>\n",
       "      <td>0.824239</td>\n",
       "      <td>0.816164</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.165379</td>\n",
       "      <td>0.057432</td>\n",
       "      <td>0.141910</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.809131</td>\n",
       "      <td>0.814204</td>\n",
       "      <td>0.812817</td>\n",
       "      <td>0.822970</td>\n",
       "      <td>0.819797</td>\n",
       "      <td>0.815784</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.879986</td>\n",
       "      <td>0.016832</td>\n",
       "      <td>0.141890</td>\n",
       "      <td>0.005257</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.808497</td>\n",
       "      <td>0.812302</td>\n",
       "      <td>0.811548</td>\n",
       "      <td>0.824239</td>\n",
       "      <td>0.817259</td>\n",
       "      <td>0.814769</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.876696</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.126755</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.805961</td>\n",
       "      <td>0.811668</td>\n",
       "      <td>0.812817</td>\n",
       "      <td>0.822970</td>\n",
       "      <td>0.817259</td>\n",
       "      <td>0.814135</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.364936</td>\n",
       "      <td>0.084261</td>\n",
       "      <td>0.140669</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.810399</td>\n",
       "      <td>0.810914</td>\n",
       "      <td>0.821701</td>\n",
       "      <td>0.817893</td>\n",
       "      <td>0.813627</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.345575</td>\n",
       "      <td>0.097333</td>\n",
       "      <td>0.123465</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.802790</td>\n",
       "      <td>0.812936</td>\n",
       "      <td>0.810914</td>\n",
       "      <td>0.820431</td>\n",
       "      <td>0.814086</td>\n",
       "      <td>0.812232</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.174914</td>\n",
       "      <td>0.034177</td>\n",
       "      <td>0.126713</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.802790</td>\n",
       "      <td>0.811668</td>\n",
       "      <td>0.810914</td>\n",
       "      <td>0.821701</td>\n",
       "      <td>0.812817</td>\n",
       "      <td>0.811978</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.228541</td>\n",
       "      <td>0.057624</td>\n",
       "      <td>0.144126</td>\n",
       "      <td>0.005073</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.807863</td>\n",
       "      <td>0.814838</td>\n",
       "      <td>0.805203</td>\n",
       "      <td>0.820431</td>\n",
       "      <td>0.810914</td>\n",
       "      <td>0.811850</td>\n",
       "      <td>0.005358</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.084094</td>\n",
       "      <td>0.113762</td>\n",
       "      <td>0.138516</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.804692</td>\n",
       "      <td>0.813570</td>\n",
       "      <td>0.807741</td>\n",
       "      <td>0.822970</td>\n",
       "      <td>0.808376</td>\n",
       "      <td>0.811470</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.911827</td>\n",
       "      <td>0.007995</td>\n",
       "      <td>0.144189</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.807863</td>\n",
       "      <td>0.813570</td>\n",
       "      <td>0.806472</td>\n",
       "      <td>0.819162</td>\n",
       "      <td>0.810279</td>\n",
       "      <td>0.811469</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.570428</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.230439</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.784401</td>\n",
       "      <td>0.788840</td>\n",
       "      <td>0.779188</td>\n",
       "      <td>0.777284</td>\n",
       "      <td>0.785533</td>\n",
       "      <td>0.783049</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.542707</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.210689</td>\n",
       "      <td>0.006232</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.778694</td>\n",
       "      <td>0.796449</td>\n",
       "      <td>0.770939</td>\n",
       "      <td>0.772208</td>\n",
       "      <td>0.779188</td>\n",
       "      <td>0.779496</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.412460</td>\n",
       "      <td>4.107624</td>\n",
       "      <td>0.233427</td>\n",
       "      <td>0.007610</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.780596</td>\n",
       "      <td>0.786303</td>\n",
       "      <td>0.774746</td>\n",
       "      <td>0.769670</td>\n",
       "      <td>0.782360</td>\n",
       "      <td>0.778735</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.572830</td>\n",
       "      <td>0.010168</td>\n",
       "      <td>0.219803</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.780596</td>\n",
       "      <td>0.787571</td>\n",
       "      <td>0.781091</td>\n",
       "      <td>0.762056</td>\n",
       "      <td>0.780457</td>\n",
       "      <td>0.778354</td>\n",
       "      <td>0.008574</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.539851</td>\n",
       "      <td>0.017494</td>\n",
       "      <td>0.220709</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.763475</td>\n",
       "      <td>0.774889</td>\n",
       "      <td>0.781726</td>\n",
       "      <td>0.782995</td>\n",
       "      <td>0.784264</td>\n",
       "      <td>0.777470</td>\n",
       "      <td>0.007711</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.020909</td>\n",
       "      <td>2.421521</td>\n",
       "      <td>0.212492</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.773621</td>\n",
       "      <td>0.796449</td>\n",
       "      <td>0.769670</td>\n",
       "      <td>0.765228</td>\n",
       "      <td>0.780457</td>\n",
       "      <td>0.777085</td>\n",
       "      <td>0.010899</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.137221</td>\n",
       "      <td>2.974325</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>0.004740</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.766646</td>\n",
       "      <td>0.781230</td>\n",
       "      <td>0.772843</td>\n",
       "      <td>0.758883</td>\n",
       "      <td>0.774746</td>\n",
       "      <td>0.770870</td>\n",
       "      <td>0.007587</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.530836</td>\n",
       "      <td>0.009038</td>\n",
       "      <td>0.211129</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.761573</td>\n",
       "      <td>0.785035</td>\n",
       "      <td>0.767132</td>\n",
       "      <td>0.763959</td>\n",
       "      <td>0.776015</td>\n",
       "      <td>0.770743</td>\n",
       "      <td>0.008663</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.662072</td>\n",
       "      <td>3.091973</td>\n",
       "      <td>0.221649</td>\n",
       "      <td>0.009205</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.753329</td>\n",
       "      <td>0.764743</td>\n",
       "      <td>0.781091</td>\n",
       "      <td>0.771574</td>\n",
       "      <td>0.775381</td>\n",
       "      <td>0.769224</td>\n",
       "      <td>0.009560</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.546952</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>0.230913</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.755866</td>\n",
       "      <td>0.775523</td>\n",
       "      <td>0.772843</td>\n",
       "      <td>0.763325</td>\n",
       "      <td>0.765863</td>\n",
       "      <td>0.766684</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.565191</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.221562</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.774255</td>\n",
       "      <td>0.774746</td>\n",
       "      <td>0.754442</td>\n",
       "      <td>0.770305</td>\n",
       "      <td>0.766557</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.514352</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.220844</td>\n",
       "      <td>0.007985</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.744451</td>\n",
       "      <td>0.762207</td>\n",
       "      <td>0.783629</td>\n",
       "      <td>0.760787</td>\n",
       "      <td>0.761421</td>\n",
       "      <td>0.762499</td>\n",
       "      <td>0.012461</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.269177</td>\n",
       "      <td>0.006230</td>\n",
       "      <td>0.128854</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.691820</td>\n",
       "      <td>0.682308</td>\n",
       "      <td>0.699873</td>\n",
       "      <td>0.699239</td>\n",
       "      <td>0.689086</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.258042</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>0.144039</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.694990</td>\n",
       "      <td>0.688015</td>\n",
       "      <td>0.696066</td>\n",
       "      <td>0.690355</td>\n",
       "      <td>0.675761</td>\n",
       "      <td>0.689038</td>\n",
       "      <td>0.007265</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.296966</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.694356</td>\n",
       "      <td>0.689918</td>\n",
       "      <td>0.697335</td>\n",
       "      <td>0.671954</td>\n",
       "      <td>0.675761</td>\n",
       "      <td>0.685865</td>\n",
       "      <td>0.010156</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.311291</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.131830</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.681040</td>\n",
       "      <td>0.682308</td>\n",
       "      <td>0.696701</td>\n",
       "      <td>0.677665</td>\n",
       "      <td>0.672589</td>\n",
       "      <td>0.682060</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.308782</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.223707</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.680406</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.689721</td>\n",
       "      <td>0.661802</td>\n",
       "      <td>0.662437</td>\n",
       "      <td>0.672671</td>\n",
       "      <td>0.010835</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.300610</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>0.233545</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.677869</td>\n",
       "      <td>0.665187</td>\n",
       "      <td>0.678299</td>\n",
       "      <td>0.657360</td>\n",
       "      <td>0.663071</td>\n",
       "      <td>0.668357</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.267206</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.213585</td>\n",
       "      <td>0.005939</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.666455</td>\n",
       "      <td>0.679772</td>\n",
       "      <td>0.673858</td>\n",
       "      <td>0.642132</td>\n",
       "      <td>0.666878</td>\n",
       "      <td>0.665819</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.259343</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.223978</td>\n",
       "      <td>0.007295</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.663285</td>\n",
       "      <td>0.661382</td>\n",
       "      <td>0.689721</td>\n",
       "      <td>0.644036</td>\n",
       "      <td>0.665609</td>\n",
       "      <td>0.664807</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "16       0.894754      0.015393         0.128359        0.006513   \n",
       "8        1.204700      0.038466         0.127841        0.005909   \n",
       "0        8.041098      0.136050         0.124596        0.004980   \n",
       "11       1.165379      0.057432         0.141910        0.005705   \n",
       "19       0.879986      0.016832         0.141890        0.005257   \n",
       "18       0.876696      0.005650         0.126755        0.005011   \n",
       "3        7.364936      0.084261         0.140669        0.004977   \n",
       "2        7.345575      0.097333         0.123465        0.004625   \n",
       "10       1.174914      0.034177         0.126713        0.004896   \n",
       "9        1.228541      0.057624         0.144126        0.005073   \n",
       "1        8.084094      0.113762         0.138516        0.005255   \n",
       "17       0.911827      0.007995         0.144189        0.005818   \n",
       "13       0.570428      0.006255         0.230439        0.006296   \n",
       "14       0.542707      0.013958         0.210689        0.006232   \n",
       "5       20.412460      4.107624         0.233427        0.007610   \n",
       "12       0.572830      0.010168         0.219803        0.006245   \n",
       "15       0.539851      0.017494         0.220709        0.008019   \n",
       "6       21.020909      2.421521         0.212492        0.006694   \n",
       "4       23.137221      2.974325         0.222058        0.004740   \n",
       "22       0.530836      0.009038         0.211129        0.006634   \n",
       "7       16.662072      3.091973         0.221649        0.009205   \n",
       "21       0.546952      0.005161         0.230913        0.006761   \n",
       "20       0.565191      0.002558         0.221562        0.006666   \n",
       "23       0.514352      0.010060         0.220844        0.007985   \n",
       "26       0.269177      0.006230         0.128854        0.004502   \n",
       "27       0.258042      0.007064         0.144039        0.004659   \n",
       "25       0.296966      0.005391         0.146939        0.003771   \n",
       "24       0.311291      0.005412         0.131830        0.006015   \n",
       "28       0.308782      0.004956         0.223707        0.006357   \n",
       "29       0.300610      0.005744         0.233545        0.007533   \n",
       "30       0.267206      0.003088         0.213585        0.005939   \n",
       "31       0.259343      0.003595         0.223978        0.007295   \n",
       "\n",
       "                                     param_classifier param_classifier__C  \\\n",
       "16  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "8   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "0   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "11  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "19  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "18  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "3   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "2   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "10  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "9   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "1   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "17  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "13  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "14  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "5   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "12  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "15  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "6   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "4   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "22  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "7   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "21  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "20  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "23  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "26       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "27       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "25       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "24       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "28       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "29       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "30       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "31       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "\n",
       "                                    param_dim_reducer  \\\n",
       "16  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "8   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "0   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "11  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "19  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "18  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "3   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "2   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "10  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "9   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "1   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "17  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "13  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "14  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "5   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "12  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "15  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "6   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "4   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "22  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "7   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "21  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "20  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "23  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "26  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "27  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "25  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "24  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "28  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "29  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "30  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "31  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "\n",
       "   param_dim_reducer__n_components param_vectorizer__min_df  \\\n",
       "16                              50                        3   \n",
       "8                               50                        3   \n",
       "0                               50                        3   \n",
       "11                              50                        5   \n",
       "19                              50                        5   \n",
       "18                              50                        5   \n",
       "3                               50                        5   \n",
       "2                               50                        5   \n",
       "10                              50                        5   \n",
       "9                               50                        3   \n",
       "1                               50                        3   \n",
       "17                              50                        3   \n",
       "13                              50                        3   \n",
       "14                              50                        5   \n",
       "5                               50                        3   \n",
       "12                              50                        3   \n",
       "15                              50                        5   \n",
       "6                               50                        5   \n",
       "4                               50                        3   \n",
       "22                              50                        5   \n",
       "7                               50                        5   \n",
       "21                              50                        3   \n",
       "20                              50                        3   \n",
       "23                              50                        5   \n",
       "26                              50                        5   \n",
       "27                              50                        5   \n",
       "25                              50                        3   \n",
       "24                              50                        3   \n",
       "28                              50                        3   \n",
       "29                              50                        3   \n",
       "30                              50                        5   \n",
       "31                              50                        5   \n",
       "\n",
       "      param_vectorizer__token_pattern  ...  \\\n",
       "16                      (?u)\\b\\w\\w+\\b  ...   \n",
       "8                       (?u)\\b\\w\\w+\\b  ...   \n",
       "0                       (?u)\\b\\w\\w+\\b  ...   \n",
       "11  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "19  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "18                      (?u)\\b\\w\\w+\\b  ...   \n",
       "3   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "2                       (?u)\\b\\w\\w+\\b  ...   \n",
       "10                      (?u)\\b\\w\\w+\\b  ...   \n",
       "9   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "1   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "17  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "13  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "14                      (?u)\\b\\w\\w+\\b  ...   \n",
       "5   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "12                      (?u)\\b\\w\\w+\\b  ...   \n",
       "15  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "6                       (?u)\\b\\w\\w+\\b  ...   \n",
       "4                       (?u)\\b\\w\\w+\\b  ...   \n",
       "22                      (?u)\\b\\w\\w+\\b  ...   \n",
       "7   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "21  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "20                      (?u)\\b\\w\\w+\\b  ...   \n",
       "23  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "26                      (?u)\\b\\w\\w+\\b  ...   \n",
       "27  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "25  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "24                      (?u)\\b\\w\\w+\\b  ...   \n",
       "28                      (?u)\\b\\w\\w+\\b  ...   \n",
       "29  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "30                      (?u)\\b\\w\\w+\\b  ...   \n",
       "31  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "\n",
       "                                               params split0_test_score  \\\n",
       "16  {'classifier': LogisticRegression(C=100, class...          0.807229   \n",
       "8   {'classifier': LogisticRegression(C=100, class...          0.804692   \n",
       "0   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.804692   \n",
       "11  {'classifier': LogisticRegression(C=100, class...          0.809131   \n",
       "19  {'classifier': LogisticRegression(C=100, class...          0.808497   \n",
       "18  {'classifier': LogisticRegression(C=100, class...          0.805961   \n",
       "3   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.807229   \n",
       "2   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.802790   \n",
       "10  {'classifier': LogisticRegression(C=100, class...          0.802790   \n",
       "9   {'classifier': LogisticRegression(C=100, class...          0.807863   \n",
       "1   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.804692   \n",
       "17  {'classifier': LogisticRegression(C=100, class...          0.807863   \n",
       "13  {'classifier': LogisticRegression(C=100, class...          0.784401   \n",
       "14  {'classifier': LogisticRegression(C=100, class...          0.778694   \n",
       "5   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.780596   \n",
       "12  {'classifier': LogisticRegression(C=100, class...          0.780596   \n",
       "15  {'classifier': LogisticRegression(C=100, class...          0.763475   \n",
       "6   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.773621   \n",
       "4   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.766646   \n",
       "22  {'classifier': LogisticRegression(C=100, class...          0.761573   \n",
       "7   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.753329   \n",
       "21  {'classifier': LogisticRegression(C=100, class...          0.755866   \n",
       "20  {'classifier': LogisticRegression(C=100, class...          0.759036   \n",
       "23  {'classifier': LogisticRegression(C=100, class...          0.744451   \n",
       "26  {'classifier': GaussianNB(priors=None, var_smo...          0.691820   \n",
       "27  {'classifier': GaussianNB(priors=None, var_smo...          0.694990   \n",
       "25  {'classifier': GaussianNB(priors=None, var_smo...          0.694356   \n",
       "24  {'classifier': GaussianNB(priors=None, var_smo...          0.681040   \n",
       "28  {'classifier': GaussianNB(priors=None, var_smo...          0.680406   \n",
       "29  {'classifier': GaussianNB(priors=None, var_smo...          0.677869   \n",
       "30  {'classifier': GaussianNB(priors=None, var_smo...          0.666455   \n",
       "31  {'classifier': GaussianNB(priors=None, var_smo...          0.663285   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "16           0.824350           0.816624           0.822970   \n",
       "8            0.824350           0.815355           0.820431   \n",
       "0            0.819911           0.812183           0.819797   \n",
       "11           0.814204           0.812817           0.822970   \n",
       "19           0.812302           0.811548           0.824239   \n",
       "18           0.811668           0.812817           0.822970   \n",
       "3            0.810399           0.810914           0.821701   \n",
       "2            0.812936           0.810914           0.820431   \n",
       "10           0.811668           0.810914           0.821701   \n",
       "9            0.814838           0.805203           0.820431   \n",
       "1            0.813570           0.807741           0.822970   \n",
       "17           0.813570           0.806472           0.819162   \n",
       "13           0.788840           0.779188           0.777284   \n",
       "14           0.796449           0.770939           0.772208   \n",
       "5            0.786303           0.774746           0.769670   \n",
       "12           0.787571           0.781091           0.762056   \n",
       "15           0.774889           0.781726           0.782995   \n",
       "6            0.796449           0.769670           0.765228   \n",
       "4            0.781230           0.772843           0.758883   \n",
       "22           0.785035           0.767132           0.763959   \n",
       "7            0.764743           0.781091           0.771574   \n",
       "21           0.775523           0.772843           0.763325   \n",
       "20           0.774255           0.774746           0.754442   \n",
       "23           0.762207           0.783629           0.760787   \n",
       "26           0.682308           0.699873           0.699239   \n",
       "27           0.688015           0.696066           0.690355   \n",
       "25           0.689918           0.697335           0.671954   \n",
       "24           0.682308           0.696701           0.677665   \n",
       "28           0.668992           0.689721           0.661802   \n",
       "29           0.665187           0.678299           0.657360   \n",
       "30           0.679772           0.673858           0.642132   \n",
       "31           0.661382           0.689721           0.644036   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  Has Header + Footer  \\\n",
       "16           0.822335         0.818702        0.006313                Fasle   \n",
       "8            0.823604         0.817687        0.007228                Fasle   \n",
       "0            0.824239         0.816164        0.006927                Fasle   \n",
       "11           0.819797         0.815784        0.004966                Fasle   \n",
       "19           0.817259         0.814769        0.005508                Fasle   \n",
       "18           0.817259         0.814135        0.005700                Fasle   \n",
       "3            0.817893         0.813627        0.005331                Fasle   \n",
       "2            0.814086         0.812232        0.005693                Fasle   \n",
       "10           0.812817         0.811978        0.006015                Fasle   \n",
       "9            0.810914         0.811850        0.005358                Fasle   \n",
       "1            0.808376         0.811470        0.006420                Fasle   \n",
       "17           0.810279         0.811469        0.004539                Fasle   \n",
       "13           0.785533         0.783049        0.004235                Fasle   \n",
       "14           0.779188         0.779496        0.009105                Fasle   \n",
       "5            0.782360         0.778735        0.005865                Fasle   \n",
       "12           0.780457         0.778354        0.008574                Fasle   \n",
       "15           0.784264         0.777470        0.007711                Fasle   \n",
       "6            0.780457         0.777085        0.010899                Fasle   \n",
       "4            0.774746         0.770870        0.007587                Fasle   \n",
       "22           0.776015         0.770743        0.008663                Fasle   \n",
       "7            0.775381         0.769224        0.009560                Fasle   \n",
       "21           0.765863         0.766684        0.007001                Fasle   \n",
       "20           0.770305         0.766557        0.008291                Fasle   \n",
       "23           0.761421         0.762499        0.012461                Fasle   \n",
       "26           0.689086         0.692465        0.006569                Fasle   \n",
       "27           0.675761         0.689038        0.007265                Fasle   \n",
       "25           0.675761         0.685865        0.010156                Fasle   \n",
       "24           0.672589         0.682060        0.008056                Fasle   \n",
       "28           0.662437         0.672671        0.010835                Fasle   \n",
       "29           0.663071         0.668357        0.008346                Fasle   \n",
       "30           0.666878         0.665819        0.012821                Fasle   \n",
       "31           0.665609         0.664807        0.014607                Fasle   \n",
       "\n",
       "   rank_test_score  \n",
       "16               1  \n",
       "8                2  \n",
       "0                3  \n",
       "11               4  \n",
       "19               5  \n",
       "18               6  \n",
       "3                7  \n",
       "2                8  \n",
       "10               9  \n",
       "9               10  \n",
       "1               11  \n",
       "17              12  \n",
       "13              13  \n",
       "14              14  \n",
       "5               15  \n",
       "12              16  \n",
       "15              17  \n",
       "6               18  \n",
       "4               19  \n",
       "22              20  \n",
       "7               21  \n",
       "21              22  \n",
       "20              23  \n",
       "23              24  \n",
       "26              25  \n",
       "27              26  \n",
       "25              27  \n",
       "24              28  \n",
       "28              29  \n",
       "29              30  \n",
       "30              31  \n",
       "31              32  \n",
       "\n",
       "[32 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################\n",
    "## Grid Search Results : DATA with HEADERS + FOOTERS ##\n",
    "#######################################################\n",
    "# Add column to Table: Used data with Header and Footers REMOVED\n",
    "table_no_hf = pd.DataFrame(grid_no_hf.cv_results_)\n",
    "table_no_hf.insert (len(table_no_hf.columns)-1, 'Has Header + Footer', 'Fasle')\n",
    "\n",
    "# Print and order by best 'accuracy'\n",
    "table_no_hf.sort_values(by=['rank_test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_dim_reducer</th>\n",
       "      <th>param_dim_reducer__n_components</th>\n",
       "      <th>param_vectorizer__min_df</th>\n",
       "      <th>param_vectorizer__token_pattern</th>\n",
       "      <th>...</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>Has Header + Footer</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.945943</td>\n",
       "      <td>0.017245</td>\n",
       "      <td>0.178805</td>\n",
       "      <td>0.005491</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.870640</td>\n",
       "      <td>0.880786</td>\n",
       "      <td>0.869289</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.861675</td>\n",
       "      <td>0.869575</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.586256</td>\n",
       "      <td>0.061375</td>\n",
       "      <td>0.177286</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.872543</td>\n",
       "      <td>0.880152</td>\n",
       "      <td>0.868020</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.861675</td>\n",
       "      <td>0.869575</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.956435</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>0.158251</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.869372</td>\n",
       "      <td>0.884591</td>\n",
       "      <td>0.862310</td>\n",
       "      <td>0.864848</td>\n",
       "      <td>0.862310</td>\n",
       "      <td>0.868686</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.580110</td>\n",
       "      <td>0.096306</td>\n",
       "      <td>0.153366</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.870640</td>\n",
       "      <td>0.884591</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.862310</td>\n",
       "      <td>0.868686</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.784404</td>\n",
       "      <td>0.043562</td>\n",
       "      <td>0.173576</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.873811</td>\n",
       "      <td>0.877616</td>\n",
       "      <td>0.869924</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.855330</td>\n",
       "      <td>0.868433</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.605547</td>\n",
       "      <td>0.070769</td>\n",
       "      <td>0.154724</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.871275</td>\n",
       "      <td>0.871275</td>\n",
       "      <td>0.861041</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.871827</td>\n",
       "      <td>0.868180</td>\n",
       "      <td>0.004259</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.988868</td>\n",
       "      <td>0.061038</td>\n",
       "      <td>0.158344</td>\n",
       "      <td>0.005003</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.869372</td>\n",
       "      <td>0.885859</td>\n",
       "      <td>0.864848</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.857868</td>\n",
       "      <td>0.868178</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.899801</td>\n",
       "      <td>0.013383</td>\n",
       "      <td>0.156915</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.869372</td>\n",
       "      <td>0.871909</td>\n",
       "      <td>0.863579</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.872462</td>\n",
       "      <td>0.868053</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.151419</td>\n",
       "      <td>0.114914</td>\n",
       "      <td>0.155776</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.870006</td>\n",
       "      <td>0.871909</td>\n",
       "      <td>0.861675</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.869924</td>\n",
       "      <td>0.867799</td>\n",
       "      <td>0.003719</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.486450</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0.175620</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.866836</td>\n",
       "      <td>0.880152</td>\n",
       "      <td>0.865482</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>0.866276</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "17       0.945943      0.017245         0.178805        0.005491   \n",
       "9        1.586256      0.061375         0.177286        0.005182   \n",
       "16       0.956435      0.013438         0.158251        0.003820   \n",
       "8        1.580110      0.096306         0.153366        0.003977   \n",
       "1        7.784404      0.043562         0.173576        0.004308   \n",
       "10       1.605547      0.070769         0.154724        0.003794   \n",
       "0        7.988868      0.061038         0.158344        0.005003   \n",
       "18       0.899801      0.013383         0.156915        0.003560   \n",
       "2        7.151419      0.114914         0.155776        0.004122   \n",
       "11       1.486450      0.044939         0.175620        0.003588   \n",
       "\n",
       "                                     param_classifier param_classifier__C  \\\n",
       "17  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "9   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "16  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "8   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "1   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "10  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "0   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "18  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "2   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "11  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "\n",
       "                                    param_dim_reducer  \\\n",
       "17  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "9   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "16  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "8   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "1   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "10  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "0   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "18  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "2   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "11  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "\n",
       "   param_dim_reducer__n_components param_vectorizer__min_df  \\\n",
       "17                              50                        3   \n",
       "9                               50                        3   \n",
       "16                              50                        3   \n",
       "8                               50                        3   \n",
       "1                               50                        3   \n",
       "10                              50                        5   \n",
       "0                               50                        3   \n",
       "18                              50                        5   \n",
       "2                               50                        5   \n",
       "11                              50                        5   \n",
       "\n",
       "      param_vectorizer__token_pattern  ...  \\\n",
       "17  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "9   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "16                      (?u)\\b\\w\\w+\\b  ...   \n",
       "8                       (?u)\\b\\w\\w+\\b  ...   \n",
       "1   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "10                      (?u)\\b\\w\\w+\\b  ...   \n",
       "0                       (?u)\\b\\w\\w+\\b  ...   \n",
       "18                      (?u)\\b\\w\\w+\\b  ...   \n",
       "2                       (?u)\\b\\w\\w+\\b  ...   \n",
       "11  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "\n",
       "                                               params split0_test_score  \\\n",
       "17  {'classifier': LogisticRegression(C=100, class...          0.870640   \n",
       "9   {'classifier': LogisticRegression(C=100, class...          0.872543   \n",
       "16  {'classifier': LogisticRegression(C=100, class...          0.869372   \n",
       "8   {'classifier': LogisticRegression(C=100, class...          0.870640   \n",
       "1   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.873811   \n",
       "10  {'classifier': LogisticRegression(C=100, class...          0.871275   \n",
       "0   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.869372   \n",
       "18  {'classifier': LogisticRegression(C=100, class...          0.869372   \n",
       "2   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.870006   \n",
       "11  {'classifier': LogisticRegression(C=100, class...          0.866836   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "17           0.880786           0.869289           0.865482   \n",
       "9            0.880152           0.868020           0.865482   \n",
       "16           0.884591           0.862310           0.864848   \n",
       "8            0.884591           0.862944           0.862944   \n",
       "1            0.877616           0.869924           0.865482   \n",
       "10           0.871275           0.861041           0.865482   \n",
       "0            0.885859           0.864848           0.862944   \n",
       "18           0.871909           0.863579           0.862944   \n",
       "2            0.871909           0.861675           0.865482   \n",
       "11           0.880152           0.865482           0.862944   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  Has Header + Footer  \\\n",
       "17           0.861675         0.869575        0.006420                 True   \n",
       "9            0.861675         0.869575        0.006360                 True   \n",
       "16           0.862310         0.868686        0.008360                 True   \n",
       "8            0.862310         0.868686        0.008525                 True   \n",
       "1            0.855330         0.868433        0.007692                 True   \n",
       "10           0.871827         0.868180        0.004259                 True   \n",
       "0            0.857868         0.868178        0.009579                 True   \n",
       "18           0.872462         0.868053        0.004054                 True   \n",
       "2            0.869924         0.867799        0.003719                 True   \n",
       "11           0.855964         0.866276        0.007886                 True   \n",
       "\n",
       "   rank_test_score  \n",
       "17               1  \n",
       "9                2  \n",
       "16               3  \n",
       "8                4  \n",
       "1                5  \n",
       "10               6  \n",
       "0                7  \n",
       "18               8  \n",
       "2                9  \n",
       "11              10  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################################################\n",
    "## Combine Findings of both Tables (W/wo) HEADERS + FOOTERS IN DATASET ##\n",
    "#########################################################################\n",
    "\n",
    "# Combine Both Tables\n",
    "combined_table = pd.concat([table_no_hf, table_hf])\n",
    "combined_table.sort_values(by=['mean_test_score'], ascending=False).head(10)\n",
    "\n",
    "## Note: Still some weirdness when combining tables. Overlapping indexes for ranking\n",
    "## Using data without headers/foots results in incredibly low scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
