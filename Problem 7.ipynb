{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Problem Setup/Definition:\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction import text, stop_words\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.svm import LinearSVC\n",
    "import math \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
      "[6 7 4 ... 6 6 2]\n",
      "First 10 articles Classification (Train): \n",
      "[1, 1, 1, 0, 0, 0, 0, 1, 1, 0]\n",
      "First 10 articles Classification (Train): \n",
      "[1, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def my_custom_preprocessor(doc_string):\n",
    "    # do all data preprocessing here\n",
    "    \n",
    "    # Lower case\n",
    "    doc_string=doc_string.lower()\n",
    "    \n",
    "    # Remove Numbers\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    doc_string.translate(remove_digits)\n",
    "    \n",
    "    # Convert to tokenized form....\n",
    "    tokens = nltk.tokenize.word_tokenize(doc_string)\n",
    "    # Iterate through list of tokens (words) and remove all numbers\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Iterate through list of tokens (words) and stem (shorten) each word\n",
    "    port_stemmer = PorterStemmer()\n",
    "    tokens = [port_stemmer.stem(words) for words in tokens ]\n",
    "    \n",
    "    ###############################\n",
    "    #### Lemmatize with pos_tag ###\n",
    "    ###############################\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Convert between two different tagging schemes\n",
    "    def change_tags(penntag):\n",
    "        morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                      'VB':'v', 'RB':'r'}\n",
    "        try:\n",
    "            return morphy_tag[penntag[:2]]\n",
    "        except:\n",
    "            return 'n'\n",
    "        \n",
    "    tokens = [lemmatizer.lemmatize(word.lower(), pos=change_tags(tag)) for word, tag in pos_tag(tokens)]\n",
    "    \n",
    "    # Rejoin List of tokens and return that single document-string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "###########################\n",
    "#### RoC Curve Function ###\n",
    "###########################\n",
    "\n",
    "def plot_roc(fpr, tpr):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    ax.plot(fpr, tpr, lw=2, label= 'area under curve = %0.4f' % roc_auc)\n",
    "\n",
    "    ax.grid(color='0.7', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate',fontsize=15)\n",
    "    ax.set_ylabel('True Positive Rate',fontsize=15)\n",
    "\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    for label in ax.get_xticklabels()+ax.get_yticklabels():\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "def fit_predict_and_plot_roc(pipe, train_data, train_label, test_data, test_label):\n",
    "    pipe.fit(train_data, train_label)\n",
    "\n",
    "    if hasattr(pipe, 'decision_function'):\n",
    "        prob_score = pipe.decision_function(test_data)\n",
    "        fpr, tpr, _ = roc_curve(test_label, prob_score)\n",
    "    else:\n",
    "        prob_score = pipe.predict_proba(test_data)\n",
    "        fpr, tpr, _ = roc_curve(test_label, prob_score[:,1])\n",
    "\n",
    "    plot_roc(fpr, tpr)\n",
    "    \n",
    "#####################################################\n",
    "#### Define Custom stop words for CountVectorizer ###\n",
    "#####################################################\n",
    "\n",
    "stop_words_skt = text.ENGLISH_STOP_WORDS\n",
    "stop_words_en = stopwords.words('english')\n",
    "combined_stopwords = set.union(set(stop_words_en),set(punctuation),set(stop_words_skt))\n",
    "\n",
    "# Run stop_words through the same pre-processor as the document-matrix\n",
    "# This will apply stemmed/lemmatized stop_woirds to stemmed/lemmatized tokenized document lists\n",
    "def process_stop_words(stop_word_set):\n",
    "    doc_string = ' '.join(stop_word_set)\n",
    "    return my_custom_preprocessor(doc_string).split()\n",
    "\n",
    "################################\n",
    "#### Estimator Helper Class  ###\n",
    "################################\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]\n",
    "\n",
    "##################################\n",
    "#### Import Dataset Train/Test ###\n",
    "##################################\n",
    "\n",
    "# Only take a specific selection (8) of the 20 available categories\n",
    "categories = ['comp.graphics', 'comp.os.ms-windows.misc',\n",
    "'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "'rec.autos', 'rec.motorcycles',\n",
    "'rec.sport.baseball', 'rec.sport.hockey']\n",
    "\n",
    "# Load a training & test data sets consisting of those 8 categories\n",
    "train_dataset = fetch_20newsgroups(subset = 'train', categories = categories, shuffle = True, random_state = None)\n",
    "test_dataset = fetch_20newsgroups(subset = 'test', categories = categories, shuffle = True, random_state = None)\n",
    "\n",
    "## Load training & test data sets WITHOUT headers & footers\n",
    "train_dataset_no_hf = fetch_20newsgroups(subset = 'train', categories = categories, shuffle = True, random_state = None, remove=['headers', 'footers'])\n",
    "test_dataset_no_hf = fetch_20newsgroups(subset = 'train', categories = categories, shuffle = True, random_state = None, remove=['headers', 'footers'])\n",
    "\n",
    "print(\"\\n\\n\" + '-'*40 + \"\\n\\n\")\n",
    "\n",
    "#############################################\n",
    "#### Define Class data set arrys (0 or 1) ###\n",
    "#############################################\n",
    "# Categorize the 8 news categories into two (binary) Classes \n",
    "# 0 = computer technology\n",
    "# 1 = recreational activity\n",
    "training_data_class = [] \n",
    "test_data_class = []\n",
    "\n",
    "# Categories are mapped 0-7, (0-3) = Comp, (4-7) = Recreation\n",
    "print(train_dataset.target_names)\n",
    "print(train_dataset.target)\n",
    "\n",
    "for category in train_dataset.target:\n",
    "    if category < 4:\n",
    "        training_data_class.append(0)\n",
    "    else:\n",
    "        training_data_class.append(1)\n",
    "        \n",
    "# Reshape test dataset\n",
    "for category in test_dataset.target:\n",
    "    if category < 4:\n",
    "        test_data_class.append(0)\n",
    "    else:\n",
    "        test_data_class.append(1)\n",
    "        \n",
    "# Sanity Checks, values should all be either 1 or 0\n",
    "print(\"First 10 articles Classification (Train): \\n\" + str(training_data_class[0:10]))\n",
    "print(\"First 10 articles Classification (Train): \\n\" + str(test_data_class[0:10]))\n",
    "# This will be used for the classification categories only!!!!\n",
    "# Each data point refers to the classification of a single article in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmpro8tpnpr'\", use \"location='/tmp/tmpro8tpnpr'\" instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "## Create Pipelines for Comparison ##\n",
    "#####################################\n",
    "#enable Cachine\n",
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=0)\n",
    "\n",
    "### Initial Pipeline ###\n",
    "# These tuples() will be altered via the 'param_grid' List[]\n",
    "pipeline_hf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('dim_reducer', TruncatedSVD()),\n",
    "    ('classifier', LinearSVC(max_iter=5000)),\n",
    "],\n",
    "memory=memory\n",
    ")\n",
    "\n",
    "pipeline_no_hf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('dim_reducer', TruncatedSVD()),\n",
    "    ('classifier', LinearSVC(max_iter=5000)),\n",
    "],\n",
    "memory=memory\n",
    ")\n",
    "\n",
    "######################\n",
    "## Cross Validation ##\n",
    "######################\n",
    "# An List[] of Dictionary{key:value} parameters that will be iterated over\n",
    "# Each Dictionary{} in the List[] references different types of 'vectorizer', 'tfidf', etc.\n",
    "# Alter the range of hyperparameters within each Dictionary{} with <estimator>__<parameter>.\n",
    "# E.g. Try both 3 & 5 min_df values for CountVectorizer().... '<vectorizer>__<min_df>'': [3,5] \n",
    "\n",
    "# Options to Iterate Over:\n",
    "\n",
    "MIN_DIF = [3,5]\n",
    "# Lemm + Stemm or Defualt\n",
    "TOKEN_PATTERN = [r'(?u)\\b\\w\\w+\\b',r'(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b']\n",
    "REDUCER_OPTIONS = [TruncatedSVD(), NMF()]\n",
    "REDUCER_N_COMPONENTS = [50]\n",
    "# LinearSVC() values\n",
    "OPTIMAL_LINEAR_C_VALUE = [100]\n",
    "# LogisticRegression() Values\n",
    "LOG_REG_PENALTIES = ['l1', 'l2']\n",
    "OPTIMAL_LOG_REG_C_VALUE = [100]\n",
    "\n",
    "param_grid = [\n",
    "                { # Linear Classifier \n",
    "                    'vectorizer__min_df': MIN_DIF,\n",
    "                    'vectorizer__token_pattern': TOKEN_PATTERN,\n",
    "                    'dim_reducer': REDUCER_OPTIONS,\n",
    "                    'dim_reducer__n_components': REDUCER_N_COMPONENTS,\n",
    "                    'classifier': [LinearSVC()],\n",
    "                    'classifier__C':OPTIMAL_LINEAR_C_VALUE\n",
    "                },\n",
    "    \n",
    "                { # Logisitc Regresion\n",
    "                    'vectorizer__min_df': MIN_DIF,\n",
    "                    'vectorizer__token_pattern': TOKEN_PATTERN,\n",
    "                    'dim_reducer': REDUCER_OPTIONS,\n",
    "                    'dim_reducer__n_components': REDUCER_N_COMPONENTS,\n",
    "                    'classifier': [LogisticRegression(solver='liblinear', max_iter=5000)],\n",
    "                    'classifier__penalty': LOG_REG_PENALTIES,\n",
    "                    'classifier__C':OPTIMAL_LOG_REG_C_VALUE                        \n",
    "                },\n",
    "    \n",
    "                { # Naive Bayes Gaussian\n",
    "                    'vectorizer__min_df': MIN_DIF,\n",
    "                    'vectorizer__token_pattern': TOKEN_PATTERN,\n",
    "                    'dim_reducer': REDUCER_OPTIONS,\n",
    "                    'dim_reducer__n_components': REDUCER_N_COMPONENTS,\n",
    "                    'classifier': [GaussianNB()],                    \n",
    "                }    \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 3.21s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 3.15s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 3.30s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 3.08s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 3.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.44s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.37s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.35s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.46s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.17s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.16s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.23s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.21s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.21s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.63s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.68s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.70s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.66s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.73s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.54s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=Memory(location=/tmp/tmpro8tpnpr/joblib),\n",
       "                                steps=[('vectorizer',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words='...\n",
       "                                              init=None, l1_ratio=0.0,\n",
       "                                              max_iter=200, n_components=None,\n",
       "                                              random_state=None, shuffle=False,\n",
       "                                              solver='cd', tol=0.0001,\n",
       "                                              verbose=0)],\n",
       "                          'dim_reducer__n_components': [50],\n",
       "                          'vectorizer__min_df': [3, 5],\n",
       "                          'vectorizer__token_pattern': ['(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                        '(?u)\\\\b[^\\\\W\\\\d_][^\\\\W\\\\d_][^\\\\W\\\\d_]+\\\\b']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cross Validate/iterate over pipeline; data has header/footer included\n",
    "grid = GridSearchCV(pipeline_hf, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid.fit(train_dataset.data, train_dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.71s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.74s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.63s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.70s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.03s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.00s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.03s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.96s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 2.06s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.85s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.70s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.79s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.82s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.81s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.47s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.45s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.35s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.44s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.71s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    }
   ],
   "source": [
    "## Cross Validate/iterate over pipeline; data has header/footer removed\n",
    "grid_no_hf = GridSearchCV(pipeline_no_hf, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid_no_hf.fit(train_dataset_no_hf.data, train_dataset.target)\n",
    "\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_dim_reducer</th>\n",
       "      <th>param_dim_reducer__n_components</th>\n",
       "      <th>param_vectorizer__min_df</th>\n",
       "      <th>param_vectorizer__token_pattern</th>\n",
       "      <th>...</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>Has Header + Footer</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.408059</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.193447</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.863780</td>\n",
       "      <td>0.850053</td>\n",
       "      <td>0.852008</td>\n",
       "      <td>0.856237</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.484378</td>\n",
       "      <td>0.033207</td>\n",
       "      <td>0.201690</td>\n",
       "      <td>0.004794</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.865892</td>\n",
       "      <td>0.859556</td>\n",
       "      <td>0.841438</td>\n",
       "      <td>0.849894</td>\n",
       "      <td>0.864693</td>\n",
       "      <td>0.856295</td>\n",
       "      <td>0.009327</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.871167</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>0.201438</td>\n",
       "      <td>0.007013</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.859556</td>\n",
       "      <td>0.858501</td>\n",
       "      <td>0.842495</td>\n",
       "      <td>0.850951</td>\n",
       "      <td>0.867865</td>\n",
       "      <td>0.855874</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.498781</td>\n",
       "      <td>0.081996</td>\n",
       "      <td>0.199659</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.863780</td>\n",
       "      <td>0.850053</td>\n",
       "      <td>0.852008</td>\n",
       "      <td>0.856237</td>\n",
       "      <td>0.855180</td>\n",
       "      <td>0.855452</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.954103</td>\n",
       "      <td>0.129723</td>\n",
       "      <td>0.202872</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.858501</td>\n",
       "      <td>0.858501</td>\n",
       "      <td>0.843552</td>\n",
       "      <td>0.846723</td>\n",
       "      <td>0.864693</td>\n",
       "      <td>0.854394</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.798886</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.195079</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.862724</td>\n",
       "      <td>0.847941</td>\n",
       "      <td>0.846723</td>\n",
       "      <td>0.853066</td>\n",
       "      <td>0.854123</td>\n",
       "      <td>0.852915</td>\n",
       "      <td>0.005671</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.189883</td>\n",
       "      <td>0.040266</td>\n",
       "      <td>0.187449</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.862724</td>\n",
       "      <td>0.844773</td>\n",
       "      <td>0.835095</td>\n",
       "      <td>0.853066</td>\n",
       "      <td>0.855180</td>\n",
       "      <td>0.850168</td>\n",
       "      <td>0.009460</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.697118</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.189007</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.862724</td>\n",
       "      <td>0.842661</td>\n",
       "      <td>0.834038</td>\n",
       "      <td>0.849894</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.849957</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.209839</td>\n",
       "      <td>0.032485</td>\n",
       "      <td>0.184384</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.861668</td>\n",
       "      <td>0.851109</td>\n",
       "      <td>0.831924</td>\n",
       "      <td>0.846723</td>\n",
       "      <td>0.856237</td>\n",
       "      <td>0.849532</td>\n",
       "      <td>0.010126</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.429940</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.188422</td>\n",
       "      <td>0.005113</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.855333</td>\n",
       "      <td>0.842661</td>\n",
       "      <td>0.838266</td>\n",
       "      <td>0.850951</td>\n",
       "      <td>0.858351</td>\n",
       "      <td>0.849112</td>\n",
       "      <td>0.007572</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.646897</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.185034</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.857445</td>\n",
       "      <td>0.851109</td>\n",
       "      <td>0.834038</td>\n",
       "      <td>0.847780</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.847842</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.531213</td>\n",
       "      <td>0.121537</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.855333</td>\n",
       "      <td>0.853221</td>\n",
       "      <td>0.832981</td>\n",
       "      <td>0.844609</td>\n",
       "      <td>0.852008</td>\n",
       "      <td>0.847630</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.614347</td>\n",
       "      <td>0.018574</td>\n",
       "      <td>0.273947</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.826822</td>\n",
       "      <td>0.818374</td>\n",
       "      <td>0.801268</td>\n",
       "      <td>0.811839</td>\n",
       "      <td>0.827696</td>\n",
       "      <td>0.817200</td>\n",
       "      <td>0.009866</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.798599</td>\n",
       "      <td>0.027466</td>\n",
       "      <td>0.306092</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.832101</td>\n",
       "      <td>0.819430</td>\n",
       "      <td>0.791755</td>\n",
       "      <td>0.820296</td>\n",
       "      <td>0.817125</td>\n",
       "      <td>0.816141</td>\n",
       "      <td>0.013255</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.727121</td>\n",
       "      <td>0.023210</td>\n",
       "      <td>0.283147</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.831045</td>\n",
       "      <td>0.806758</td>\n",
       "      <td>0.808668</td>\n",
       "      <td>0.816068</td>\n",
       "      <td>0.810782</td>\n",
       "      <td>0.814664</td>\n",
       "      <td>0.008762</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.575513</td>\n",
       "      <td>0.016831</td>\n",
       "      <td>0.259748</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.842661</td>\n",
       "      <td>0.797254</td>\n",
       "      <td>0.800211</td>\n",
       "      <td>0.797040</td>\n",
       "      <td>0.835095</td>\n",
       "      <td>0.814452</td>\n",
       "      <td>0.020118</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.070481</td>\n",
       "      <td>4.272459</td>\n",
       "      <td>0.279576</td>\n",
       "      <td>0.007013</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.820486</td>\n",
       "      <td>0.808870</td>\n",
       "      <td>0.791755</td>\n",
       "      <td>0.808668</td>\n",
       "      <td>0.827696</td>\n",
       "      <td>0.811495</td>\n",
       "      <td>0.012230</td>\n",
       "      <td>True</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.462240</td>\n",
       "      <td>5.118694</td>\n",
       "      <td>0.313854</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.823654</td>\n",
       "      <td>0.812038</td>\n",
       "      <td>0.780127</td>\n",
       "      <td>0.819239</td>\n",
       "      <td>0.817125</td>\n",
       "      <td>0.810436</td>\n",
       "      <td>0.015609</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.989134</td>\n",
       "      <td>3.312757</td>\n",
       "      <td>0.261963</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.822598</td>\n",
       "      <td>0.786695</td>\n",
       "      <td>0.798097</td>\n",
       "      <td>0.798097</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.806214</td>\n",
       "      <td>0.015207</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.916521</td>\n",
       "      <td>3.832389</td>\n",
       "      <td>0.284388</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.821542</td>\n",
       "      <td>0.783527</td>\n",
       "      <td>0.808668</td>\n",
       "      <td>0.804440</td>\n",
       "      <td>0.810782</td>\n",
       "      <td>0.805792</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.502825</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.276925</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.805702</td>\n",
       "      <td>0.806758</td>\n",
       "      <td>0.779070</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.828753</td>\n",
       "      <td>0.804522</td>\n",
       "      <td>0.015787</td>\n",
       "      <td>True</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.592215</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>0.283581</td>\n",
       "      <td>0.007789</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.815206</td>\n",
       "      <td>0.786695</td>\n",
       "      <td>0.798097</td>\n",
       "      <td>0.792812</td>\n",
       "      <td>0.798097</td>\n",
       "      <td>0.798181</td>\n",
       "      <td>0.009493</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.447910</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>0.259050</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.813094</td>\n",
       "      <td>0.781415</td>\n",
       "      <td>0.793869</td>\n",
       "      <td>0.786469</td>\n",
       "      <td>0.807611</td>\n",
       "      <td>0.796492</td>\n",
       "      <td>0.012115</td>\n",
       "      <td>True</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.663283</td>\n",
       "      <td>0.007550</td>\n",
       "      <td>0.310310</td>\n",
       "      <td>0.010301</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.805702</td>\n",
       "      <td>0.806758</td>\n",
       "      <td>0.766385</td>\n",
       "      <td>0.798097</td>\n",
       "      <td>0.794926</td>\n",
       "      <td>0.794374</td>\n",
       "      <td>0.014692</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.465228</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.711721</td>\n",
       "      <td>0.734952</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.702960</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.715973</td>\n",
       "      <td>0.015215</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.541731</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>0.200918</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.689546</td>\n",
       "      <td>0.730729</td>\n",
       "      <td>0.687104</td>\n",
       "      <td>0.719873</td>\n",
       "      <td>0.716702</td>\n",
       "      <td>0.708791</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.313995</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.187235</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.691658</td>\n",
       "      <td>0.729673</td>\n",
       "      <td>0.673362</td>\n",
       "      <td>0.711416</td>\n",
       "      <td>0.707188</td>\n",
       "      <td>0.702659</td>\n",
       "      <td>0.019007</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.376638</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.281044</td>\n",
       "      <td>0.006284</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.724393</td>\n",
       "      <td>0.692714</td>\n",
       "      <td>0.682875</td>\n",
       "      <td>0.682875</td>\n",
       "      <td>0.688161</td>\n",
       "      <td>0.694204</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.373937</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.190351</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.686378</td>\n",
       "      <td>0.717001</td>\n",
       "      <td>0.676533</td>\n",
       "      <td>0.680761</td>\n",
       "      <td>0.704017</td>\n",
       "      <td>0.692938</td>\n",
       "      <td>0.015246</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.264104</td>\n",
       "      <td>0.004610</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.710665</td>\n",
       "      <td>0.674762</td>\n",
       "      <td>0.683932</td>\n",
       "      <td>0.658562</td>\n",
       "      <td>0.690275</td>\n",
       "      <td>0.683639</td>\n",
       "      <td>0.017217</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.471277</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>0.287955</td>\n",
       "      <td>0.011295</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.699050</td>\n",
       "      <td>0.687434</td>\n",
       "      <td>0.668076</td>\n",
       "      <td>0.664905</td>\n",
       "      <td>0.688161</td>\n",
       "      <td>0.681525</td>\n",
       "      <td>0.012986</td>\n",
       "      <td>True</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.548457</td>\n",
       "      <td>0.010872</td>\n",
       "      <td>0.312255</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.684266</td>\n",
       "      <td>0.697994</td>\n",
       "      <td>0.653277</td>\n",
       "      <td>0.671247</td>\n",
       "      <td>0.677590</td>\n",
       "      <td>0.676875</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10       1.408059      0.090800         0.193447        0.004733   \n",
       "8        1.484378      0.033207         0.201690        0.004794   \n",
       "16       0.871167      0.005852         0.201438        0.007013   \n",
       "2        6.498781      0.081996         0.199659        0.003200   \n",
       "0        7.954103      0.129723         0.202872        0.007657   \n",
       "18       0.798886      0.007357         0.195079        0.004098   \n",
       "9        1.189883      0.040266         0.187449        0.002881   \n",
       "17       0.697118      0.008917         0.189007        0.003977   \n",
       "11       1.209839      0.032485         0.184384        0.004553   \n",
       "1        6.429940      0.061321         0.188422        0.005113   \n",
       "19       0.646897      0.011224         0.185034        0.004562   \n",
       "3        5.531213      0.121537         0.187000        0.005481   \n",
       "13       0.614347      0.018574         0.273947        0.004654   \n",
       "12       0.798599      0.027466         0.306092        0.009427   \n",
       "14       0.727121      0.023210         0.283147        0.009917   \n",
       "15       0.575513      0.016831         0.259748        0.002222   \n",
       "5       21.070481      4.272459         0.279576        0.007013   \n",
       "4       26.462240      5.118694         0.313854        0.014846   \n",
       "7       14.989134      3.312757         0.261963        0.003023   \n",
       "6       15.916521      3.832389         0.284388        0.009830   \n",
       "21       0.502825      0.005262         0.276925        0.003266   \n",
       "22       0.592215      0.009808         0.283581        0.007789   \n",
       "23       0.447910      0.007014         0.259050        0.004108   \n",
       "20       0.663283      0.007550         0.310310        0.010301   \n",
       "26       0.465228      0.005912         0.197505        0.005717   \n",
       "24       0.541731      0.006244         0.200918        0.006454   \n",
       "27       0.313995      0.003311         0.187235        0.003776   \n",
       "29       0.376638      0.003288         0.281044        0.006284   \n",
       "25       0.373937      0.003874         0.190351        0.003808   \n",
       "31       0.320980      0.007667         0.264104        0.004610   \n",
       "30       0.471277      0.008275         0.287955        0.011295   \n",
       "28       0.548457      0.010872         0.312255        0.010124   \n",
       "\n",
       "                                     param_classifier param_classifier__C  \\\n",
       "10  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "8   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "16  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "2   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "0   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "18  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "9   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "17  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "11  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "1   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "19  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "3   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "13  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "12  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "14  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "15  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "5   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "4   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "7   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "6   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "21  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "22  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "23  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "20  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "26       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "24       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "27       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "29       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "25       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "31       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "30       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "28       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "\n",
       "                                    param_dim_reducer  \\\n",
       "10  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "8   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "16  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "2   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "0   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "18  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "9   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "17  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "11  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "1   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "19  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "3   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "13  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "12  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "14  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "15  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "5   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "4   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "7   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "6   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "21  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "22  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "23  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "20  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "26  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "24  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "27  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "29  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "25  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "31  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "30  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "28  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "\n",
       "   param_dim_reducer__n_components param_vectorizer__min_df  \\\n",
       "10                              50                        5   \n",
       "8                               50                        3   \n",
       "16                              50                        3   \n",
       "2                               50                        5   \n",
       "0                               50                        3   \n",
       "18                              50                        5   \n",
       "9                               50                        3   \n",
       "17                              50                        3   \n",
       "11                              50                        5   \n",
       "1                               50                        3   \n",
       "19                              50                        5   \n",
       "3                               50                        5   \n",
       "13                              50                        3   \n",
       "12                              50                        3   \n",
       "14                              50                        5   \n",
       "15                              50                        5   \n",
       "5                               50                        3   \n",
       "4                               50                        3   \n",
       "7                               50                        5   \n",
       "6                               50                        5   \n",
       "21                              50                        3   \n",
       "22                              50                        5   \n",
       "23                              50                        5   \n",
       "20                              50                        3   \n",
       "26                              50                        5   \n",
       "24                              50                        3   \n",
       "27                              50                        5   \n",
       "29                              50                        3   \n",
       "25                              50                        3   \n",
       "31                              50                        5   \n",
       "30                              50                        5   \n",
       "28                              50                        3   \n",
       "\n",
       "      param_vectorizer__token_pattern  ...  \\\n",
       "10                      (?u)\\b\\w\\w+\\b  ...   \n",
       "8                       (?u)\\b\\w\\w+\\b  ...   \n",
       "16                      (?u)\\b\\w\\w+\\b  ...   \n",
       "2                       (?u)\\b\\w\\w+\\b  ...   \n",
       "0                       (?u)\\b\\w\\w+\\b  ...   \n",
       "18                      (?u)\\b\\w\\w+\\b  ...   \n",
       "9   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "17  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "11  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "1   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "19  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "3   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "13  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "12                      (?u)\\b\\w\\w+\\b  ...   \n",
       "14                      (?u)\\b\\w\\w+\\b  ...   \n",
       "15  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "5   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "4                       (?u)\\b\\w\\w+\\b  ...   \n",
       "7   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "6                       (?u)\\b\\w\\w+\\b  ...   \n",
       "21  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "22                      (?u)\\b\\w\\w+\\b  ...   \n",
       "23  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "20                      (?u)\\b\\w\\w+\\b  ...   \n",
       "26                      (?u)\\b\\w\\w+\\b  ...   \n",
       "24                      (?u)\\b\\w\\w+\\b  ...   \n",
       "27  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "29  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "25  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "31  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "30                      (?u)\\b\\w\\w+\\b  ...   \n",
       "28                      (?u)\\b\\w\\w+\\b  ...   \n",
       "\n",
       "                                               params split0_test_score  \\\n",
       "10  {'classifier': LogisticRegression(C=100, class...          0.863780   \n",
       "8   {'classifier': LogisticRegression(C=100, class...          0.865892   \n",
       "16  {'classifier': LogisticRegression(C=100, class...          0.859556   \n",
       "2   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.863780   \n",
       "0   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.858501   \n",
       "18  {'classifier': LogisticRegression(C=100, class...          0.862724   \n",
       "9   {'classifier': LogisticRegression(C=100, class...          0.862724   \n",
       "17  {'classifier': LogisticRegression(C=100, class...          0.862724   \n",
       "11  {'classifier': LogisticRegression(C=100, class...          0.861668   \n",
       "1   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.855333   \n",
       "19  {'classifier': LogisticRegression(C=100, class...          0.857445   \n",
       "3   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.855333   \n",
       "13  {'classifier': LogisticRegression(C=100, class...          0.826822   \n",
       "12  {'classifier': LogisticRegression(C=100, class...          0.832101   \n",
       "14  {'classifier': LogisticRegression(C=100, class...          0.831045   \n",
       "15  {'classifier': LogisticRegression(C=100, class...          0.842661   \n",
       "5   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.820486   \n",
       "4   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.823654   \n",
       "7   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.822598   \n",
       "6   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.821542   \n",
       "21  {'classifier': LogisticRegression(C=100, class...          0.805702   \n",
       "22  {'classifier': LogisticRegression(C=100, class...          0.815206   \n",
       "23  {'classifier': LogisticRegression(C=100, class...          0.813094   \n",
       "20  {'classifier': LogisticRegression(C=100, class...          0.805702   \n",
       "26  {'classifier': GaussianNB(priors=None, var_smo...          0.711721   \n",
       "24  {'classifier': GaussianNB(priors=None, var_smo...          0.689546   \n",
       "27  {'classifier': GaussianNB(priors=None, var_smo...          0.691658   \n",
       "29  {'classifier': GaussianNB(priors=None, var_smo...          0.724393   \n",
       "25  {'classifier': GaussianNB(priors=None, var_smo...          0.686378   \n",
       "31  {'classifier': GaussianNB(priors=None, var_smo...          0.710665   \n",
       "30  {'classifier': GaussianNB(priors=None, var_smo...          0.699050   \n",
       "28  {'classifier': GaussianNB(priors=None, var_smo...          0.684266   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "10           0.850053           0.852008           0.856237   \n",
       "8            0.859556           0.841438           0.849894   \n",
       "16           0.858501           0.842495           0.850951   \n",
       "2            0.850053           0.852008           0.856237   \n",
       "0            0.858501           0.843552           0.846723   \n",
       "18           0.847941           0.846723           0.853066   \n",
       "9            0.844773           0.835095           0.853066   \n",
       "17           0.842661           0.834038           0.849894   \n",
       "11           0.851109           0.831924           0.846723   \n",
       "1            0.842661           0.838266           0.850951   \n",
       "19           0.851109           0.834038           0.847780   \n",
       "3            0.853221           0.832981           0.844609   \n",
       "13           0.818374           0.801268           0.811839   \n",
       "12           0.819430           0.791755           0.820296   \n",
       "14           0.806758           0.808668           0.816068   \n",
       "15           0.797254           0.800211           0.797040   \n",
       "5            0.808870           0.791755           0.808668   \n",
       "4            0.812038           0.780127           0.819239   \n",
       "7            0.786695           0.798097           0.798097   \n",
       "6            0.783527           0.808668           0.804440   \n",
       "21           0.806758           0.779070           0.802326   \n",
       "22           0.786695           0.798097           0.792812   \n",
       "23           0.781415           0.793869           0.786469   \n",
       "20           0.806758           0.766385           0.798097   \n",
       "26           0.734952           0.697674           0.702960   \n",
       "24           0.730729           0.687104           0.719873   \n",
       "27           0.729673           0.673362           0.711416   \n",
       "29           0.692714           0.682875           0.682875   \n",
       "25           0.717001           0.676533           0.680761   \n",
       "31           0.674762           0.683932           0.658562   \n",
       "30           0.687434           0.668076           0.664905   \n",
       "28           0.697994           0.653277           0.671247   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  Has Header + Footer  \\\n",
       "10           0.863636         0.857143        0.005721                 True   \n",
       "8            0.864693         0.856295        0.009327                 True   \n",
       "16           0.867865         0.855874        0.008573                 True   \n",
       "2            0.855180         0.855452        0.004713                 True   \n",
       "0            0.864693         0.854394        0.007952                 True   \n",
       "18           0.854123         0.852915        0.005671                 True   \n",
       "9            0.855180         0.850168        0.009460                 True   \n",
       "17           0.860465         0.849957        0.010771                 True   \n",
       "11           0.856237         0.849532        0.010126                 True   \n",
       "1            0.858351         0.849112        0.007572                 True   \n",
       "19           0.848837         0.847842        0.007674                 True   \n",
       "3            0.852008         0.847630        0.008167                 True   \n",
       "13           0.827696         0.817200        0.009866                 True   \n",
       "12           0.817125         0.816141        0.013255                 True   \n",
       "14           0.810782         0.814664        0.008762                 True   \n",
       "15           0.835095         0.814452        0.020118                 True   \n",
       "5            0.827696         0.811495        0.012230                 True   \n",
       "4            0.817125         0.810436        0.015609                 True   \n",
       "7            0.825581         0.806214        0.015207                 True   \n",
       "6            0.810782         0.805792        0.012480                 True   \n",
       "21           0.828753         0.804522        0.015787                 True   \n",
       "22           0.798097         0.798181        0.009493                 True   \n",
       "23           0.807611         0.796492        0.012115                 True   \n",
       "20           0.794926         0.794374        0.014692                 True   \n",
       "26           0.732558         0.715973        0.015215                 True   \n",
       "24           0.716702         0.708791        0.017363                 True   \n",
       "27           0.707188         0.702659        0.019007                 True   \n",
       "29           0.688161         0.694204        0.015536                 True   \n",
       "25           0.704017         0.692938        0.015246                 True   \n",
       "31           0.690275         0.683639        0.017217                 True   \n",
       "30           0.688161         0.681525        0.012986                 True   \n",
       "28           0.677590         0.676875        0.014763                 True   \n",
       "\n",
       "   rank_test_score  \n",
       "10               1  \n",
       "8                2  \n",
       "16               3  \n",
       "2                4  \n",
       "0                5  \n",
       "18               6  \n",
       "9                7  \n",
       "17               8  \n",
       "11               9  \n",
       "1               10  \n",
       "19              11  \n",
       "3               12  \n",
       "13              13  \n",
       "12              14  \n",
       "14              15  \n",
       "15              16  \n",
       "5               17  \n",
       "4               18  \n",
       "7               19  \n",
       "6               20  \n",
       "21              21  \n",
       "22              22  \n",
       "23              23  \n",
       "20              24  \n",
       "26              25  \n",
       "24              26  \n",
       "27              27  \n",
       "29              28  \n",
       "25              29  \n",
       "31              30  \n",
       "30              31  \n",
       "28              32  \n",
       "\n",
       "[32 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################\n",
    "## Grid Search Results : DATA with HEADERS + FOOTERS ##\n",
    "#######################################################\n",
    "# Add column to Table: Used data with Header and Footers INCLUDED\n",
    "table_hf = pd.DataFrame(grid.cv_results_)\n",
    "table_hf.insert (len(table_hf.columns)-1, 'Has Header + Footer', 'True')\n",
    "\n",
    "# Print and order by best 'accuracy'\n",
    "table_hf.sort_values(by=['rank_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_dim_reducer</th>\n",
       "      <th>param_dim_reducer__n_components</th>\n",
       "      <th>param_vectorizer__min_df</th>\n",
       "      <th>param_vectorizer__token_pattern</th>\n",
       "      <th>...</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>Has Header + Footer</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.378391</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.202152</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.142555</td>\n",
       "      <td>0.135164</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.141649</td>\n",
       "      <td>0.137785</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.670012</td>\n",
       "      <td>0.020358</td>\n",
       "      <td>0.161506</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.139388</td>\n",
       "      <td>0.149947</td>\n",
       "      <td>0.131078</td>\n",
       "      <td>0.131078</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.136937</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.886474</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.159901</td>\n",
       "      <td>0.008201</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.140444</td>\n",
       "      <td>0.149947</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.125793</td>\n",
       "      <td>0.134249</td>\n",
       "      <td>0.136725</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.455323</td>\n",
       "      <td>0.010623</td>\n",
       "      <td>0.207047</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.133052</td>\n",
       "      <td>0.131996</td>\n",
       "      <td>0.135307</td>\n",
       "      <td>0.140592</td>\n",
       "      <td>0.141649</td>\n",
       "      <td>0.136519</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.851201</td>\n",
       "      <td>0.017299</td>\n",
       "      <td>0.156837</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.128828</td>\n",
       "      <td>0.138332</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.132135</td>\n",
       "      <td>0.145877</td>\n",
       "      <td>0.134616</td>\n",
       "      <td>0.006713</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.365302</td>\n",
       "      <td>1.582189</td>\n",
       "      <td>0.205007</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.141499</td>\n",
       "      <td>0.134108</td>\n",
       "      <td>0.121564</td>\n",
       "      <td>0.132135</td>\n",
       "      <td>0.142706</td>\n",
       "      <td>0.134403</td>\n",
       "      <td>0.007607</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.592844</td>\n",
       "      <td>0.014176</td>\n",
       "      <td>0.155059</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.127772</td>\n",
       "      <td>0.138332</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.143763</td>\n",
       "      <td>0.133559</td>\n",
       "      <td>0.006905</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.567618</td>\n",
       "      <td>0.016777</td>\n",
       "      <td>0.218685</td>\n",
       "      <td>0.009512</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.143611</td>\n",
       "      <td>0.137276</td>\n",
       "      <td>0.119450</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.132922</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.582217</td>\n",
       "      <td>0.060959</td>\n",
       "      <td>0.163484</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.136220</td>\n",
       "      <td>0.144667</td>\n",
       "      <td>0.137421</td>\n",
       "      <td>0.118393</td>\n",
       "      <td>0.125793</td>\n",
       "      <td>0.132499</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.139840</td>\n",
       "      <td>0.108874</td>\n",
       "      <td>0.148912</td>\n",
       "      <td>0.007119</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.135164</td>\n",
       "      <td>0.127772</td>\n",
       "      <td>0.131078</td>\n",
       "      <td>0.132135</td>\n",
       "      <td>0.135307</td>\n",
       "      <td>0.132291</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.079398</td>\n",
       "      <td>0.143874</td>\n",
       "      <td>0.157957</td>\n",
       "      <td>0.010793</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.131996</td>\n",
       "      <td>0.140444</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.132289</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.060695</td>\n",
       "      <td>4.602733</td>\n",
       "      <td>0.243078</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.141499</td>\n",
       "      <td>0.142555</td>\n",
       "      <td>0.117336</td>\n",
       "      <td>0.123679</td>\n",
       "      <td>0.132135</td>\n",
       "      <td>0.131441</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.495753</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.219551</td>\n",
       "      <td>0.010009</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.134108</td>\n",
       "      <td>0.131996</td>\n",
       "      <td>0.121564</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.130811</td>\n",
       "      <td>0.004671</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.464147</td>\n",
       "      <td>0.016501</td>\n",
       "      <td>0.164031</td>\n",
       "      <td>0.010452</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.111932</td>\n",
       "      <td>0.137276</td>\n",
       "      <td>0.125793</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.138478</td>\n",
       "      <td>0.130603</td>\n",
       "      <td>0.010564</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.322362</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.152286</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.123548</td>\n",
       "      <td>0.142555</td>\n",
       "      <td>0.121564</td>\n",
       "      <td>0.122622</td>\n",
       "      <td>0.138478</td>\n",
       "      <td>0.129753</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.735852</td>\n",
       "      <td>0.007769</td>\n",
       "      <td>0.148799</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.133052</td>\n",
       "      <td>0.133052</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.123679</td>\n",
       "      <td>0.134249</td>\n",
       "      <td>0.129753</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.517650</td>\n",
       "      <td>0.008764</td>\n",
       "      <td>0.148489</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.131996</td>\n",
       "      <td>0.131996</td>\n",
       "      <td>0.123679</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.134249</td>\n",
       "      <td>0.129331</td>\n",
       "      <td>0.004277</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.631402</td>\n",
       "      <td>0.017905</td>\n",
       "      <td>0.239947</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.128828</td>\n",
       "      <td>0.141499</td>\n",
       "      <td>0.113108</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.137421</td>\n",
       "      <td>0.129118</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.333229</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>0.228689</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.143611</td>\n",
       "      <td>0.128828</td>\n",
       "      <td>0.119450</td>\n",
       "      <td>0.125793</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.129118</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.269844</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.147499</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.134108</td>\n",
       "      <td>0.126716</td>\n",
       "      <td>0.119450</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.128909</td>\n",
       "      <td>0.005965</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.563216</td>\n",
       "      <td>0.020155</td>\n",
       "      <td>0.239634</td>\n",
       "      <td>0.007226</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.126716</td>\n",
       "      <td>0.136220</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.122622</td>\n",
       "      <td>0.134249</td>\n",
       "      <td>0.128908</td>\n",
       "      <td>0.005361</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.398117</td>\n",
       "      <td>0.017051</td>\n",
       "      <td>0.220420</td>\n",
       "      <td>0.009587</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.128828</td>\n",
       "      <td>0.123548</td>\n",
       "      <td>0.112051</td>\n",
       "      <td>0.138478</td>\n",
       "      <td>0.137421</td>\n",
       "      <td>0.128065</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.172029</td>\n",
       "      <td>4.300595</td>\n",
       "      <td>0.217781</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.118268</td>\n",
       "      <td>0.141499</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.128964</td>\n",
       "      <td>0.127641</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16.374611</td>\n",
       "      <td>2.703669</td>\n",
       "      <td>0.221115</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.127772</td>\n",
       "      <td>0.133052</td>\n",
       "      <td>0.117336</td>\n",
       "      <td>0.125793</td>\n",
       "      <td>0.134249</td>\n",
       "      <td>0.127640</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.467424</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.247353</td>\n",
       "      <td>0.010335</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.121436</td>\n",
       "      <td>0.121436</td>\n",
       "      <td>0.130021</td>\n",
       "      <td>0.138478</td>\n",
       "      <td>0.123679</td>\n",
       "      <td>0.127010</td>\n",
       "      <td>0.006539</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.498140</td>\n",
       "      <td>0.014438</td>\n",
       "      <td>0.213683</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.119324</td>\n",
       "      <td>0.140444</td>\n",
       "      <td>0.114165</td>\n",
       "      <td>0.131078</td>\n",
       "      <td>0.128964</td>\n",
       "      <td>0.126795</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.331931</td>\n",
       "      <td>0.123331</td>\n",
       "      <td>0.148830</td>\n",
       "      <td>0.011985</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>0.128828</td>\n",
       "      <td>0.125660</td>\n",
       "      <td>0.122622</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.125316</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.276968</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.208154</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.125660</td>\n",
       "      <td>0.118268</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.140592</td>\n",
       "      <td>0.125107</td>\n",
       "      <td>0.008543</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.467830</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.147230</td>\n",
       "      <td>0.006690</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.129884</td>\n",
       "      <td>0.125660</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.118393</td>\n",
       "      <td>0.118393</td>\n",
       "      <td>0.123413</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.721603</td>\n",
       "      <td>0.038945</td>\n",
       "      <td>0.149164</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.129884</td>\n",
       "      <td>0.125660</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.118393</td>\n",
       "      <td>0.118393</td>\n",
       "      <td>0.123413</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.395729</td>\n",
       "      <td>0.019597</td>\n",
       "      <td>0.158933</td>\n",
       "      <td>0.007624</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.106653</td>\n",
       "      <td>0.127772</td>\n",
       "      <td>0.119450</td>\n",
       "      <td>0.123679</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.122149</td>\n",
       "      <td>0.008981</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.425403</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.215795</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.110876</td>\n",
       "      <td>0.142555</td>\n",
       "      <td>0.113108</td>\n",
       "      <td>0.119450</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.122145</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "23       0.378391      0.006370         0.202152        0.007484   \n",
       "16       0.670012      0.020358         0.161506        0.007492   \n",
       "8        0.886474      0.024418         0.159901        0.008201   \n",
       "15       0.455323      0.010623         0.207047        0.007051   \n",
       "10       0.851201      0.017299         0.156837        0.008664   \n",
       "7       14.365302      1.582189         0.205007        0.007871   \n",
       "18       0.592844      0.014176         0.155059        0.008051   \n",
       "14       0.567618      0.016777         0.218685        0.009512   \n",
       "0       11.582217      0.060959         0.163484        0.005727   \n",
       "1       10.139840      0.108874         0.148912        0.007119   \n",
       "2       10.079398      0.143874         0.157957        0.010793   \n",
       "4       18.060695      4.602733         0.243078        0.008499   \n",
       "22       0.495753      0.017182         0.219551        0.010009   \n",
       "24       0.464147      0.016501         0.164031        0.010452   \n",
       "25       0.322362      0.007512         0.152286        0.006579   \n",
       "9        0.735852      0.007769         0.148799        0.006980   \n",
       "17       0.517650      0.008764         0.148489        0.006593   \n",
       "12       0.631402      0.017905         0.239947        0.007539   \n",
       "29       0.333229      0.006752         0.228689        0.007334   \n",
       "27       0.269844      0.003732         0.147499        0.006766   \n",
       "20       0.563216      0.020155         0.239634        0.007226   \n",
       "30       0.398117      0.017051         0.220420        0.009587   \n",
       "5       17.172029      4.300595         0.217781        0.007423   \n",
       "6       16.374611      2.703669         0.221115        0.008801   \n",
       "28       0.467424      0.019192         0.247353        0.010335   \n",
       "13       0.498140      0.014438         0.213683        0.007533   \n",
       "3        9.331931      0.123331         0.148830        0.011985   \n",
       "31       0.276968      0.006380         0.208154        0.008411   \n",
       "19       0.467830      0.007138         0.147230        0.006690   \n",
       "11       0.721603      0.038945         0.149164        0.008442   \n",
       "26       0.395729      0.019597         0.158933        0.007624   \n",
       "21       0.425403      0.008214         0.215795        0.008208   \n",
       "\n",
       "                                     param_classifier param_classifier__C  \\\n",
       "23  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "16  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "8   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "15  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "10  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "7   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "18  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "14  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "0   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "1   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "2   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "4   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "22  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "24       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "25       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "9   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "17  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "12  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "29       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "27       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "20  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "30       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "5   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "6   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "28       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "13  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "3   LinearSVC(C=1.0, class_weight=None, dual=True,...                 100   \n",
       "31       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "19  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "11  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "26       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "21  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "\n",
       "                                    param_dim_reducer  \\\n",
       "23  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "16  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "8   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "15  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "10  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "7   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "18  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "14  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "0   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "1   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "2   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "4   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "22  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "24  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "25  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "9   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "17  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "12  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "29  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "27  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "20  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "30  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "5   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "6   NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "28  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "13  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "3   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "31  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "19  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "11  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "26  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "21  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "\n",
       "   param_dim_reducer__n_components param_vectorizer__min_df  \\\n",
       "23                              50                        5   \n",
       "16                              50                        3   \n",
       "8                               50                        3   \n",
       "15                              50                        5   \n",
       "10                              50                        5   \n",
       "7                               50                        5   \n",
       "18                              50                        5   \n",
       "14                              50                        5   \n",
       "0                               50                        3   \n",
       "1                               50                        3   \n",
       "2                               50                        5   \n",
       "4                               50                        3   \n",
       "22                              50                        5   \n",
       "24                              50                        3   \n",
       "25                              50                        3   \n",
       "9                               50                        3   \n",
       "17                              50                        3   \n",
       "12                              50                        3   \n",
       "29                              50                        3   \n",
       "27                              50                        5   \n",
       "20                              50                        3   \n",
       "30                              50                        5   \n",
       "5                               50                        3   \n",
       "6                               50                        5   \n",
       "28                              50                        3   \n",
       "13                              50                        3   \n",
       "3                               50                        5   \n",
       "31                              50                        5   \n",
       "19                              50                        5   \n",
       "11                              50                        5   \n",
       "26                              50                        5   \n",
       "21                              50                        3   \n",
       "\n",
       "      param_vectorizer__token_pattern  ...  \\\n",
       "23  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "16                      (?u)\\b\\w\\w+\\b  ...   \n",
       "8                       (?u)\\b\\w\\w+\\b  ...   \n",
       "15  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "10                      (?u)\\b\\w\\w+\\b  ...   \n",
       "7   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "18                      (?u)\\b\\w\\w+\\b  ...   \n",
       "14                      (?u)\\b\\w\\w+\\b  ...   \n",
       "0                       (?u)\\b\\w\\w+\\b  ...   \n",
       "1   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "2                       (?u)\\b\\w\\w+\\b  ...   \n",
       "4                       (?u)\\b\\w\\w+\\b  ...   \n",
       "22                      (?u)\\b\\w\\w+\\b  ...   \n",
       "24                      (?u)\\b\\w\\w+\\b  ...   \n",
       "25  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "9   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "17  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "12                      (?u)\\b\\w\\w+\\b  ...   \n",
       "29  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "27  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "20                      (?u)\\b\\w\\w+\\b  ...   \n",
       "30                      (?u)\\b\\w\\w+\\b  ...   \n",
       "5   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "6                       (?u)\\b\\w\\w+\\b  ...   \n",
       "28                      (?u)\\b\\w\\w+\\b  ...   \n",
       "13  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "3   (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "31  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "19  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "11  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "26                      (?u)\\b\\w\\w+\\b  ...   \n",
       "21  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "\n",
       "                                               params split0_test_score  \\\n",
       "23  {'classifier': LogisticRegression(C=100, class...          0.142555   \n",
       "16  {'classifier': LogisticRegression(C=100, class...          0.139388   \n",
       "8   {'classifier': LogisticRegression(C=100, class...          0.140444   \n",
       "15  {'classifier': LogisticRegression(C=100, class...          0.133052   \n",
       "10  {'classifier': LogisticRegression(C=100, class...          0.128828   \n",
       "7   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.141499   \n",
       "18  {'classifier': LogisticRegression(C=100, class...          0.127772   \n",
       "14  {'classifier': LogisticRegression(C=100, class...          0.143611   \n",
       "0   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.136220   \n",
       "1   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.135164   \n",
       "2   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.131996   \n",
       "4   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.141499   \n",
       "22  {'classifier': LogisticRegression(C=100, class...          0.134108   \n",
       "24  {'classifier': GaussianNB(priors=None, var_smo...          0.111932   \n",
       "25  {'classifier': GaussianNB(priors=None, var_smo...          0.123548   \n",
       "9   {'classifier': LogisticRegression(C=100, class...          0.133052   \n",
       "17  {'classifier': LogisticRegression(C=100, class...          0.131996   \n",
       "12  {'classifier': LogisticRegression(C=100, class...          0.128828   \n",
       "29  {'classifier': GaussianNB(priors=None, var_smo...          0.143611   \n",
       "27  {'classifier': GaussianNB(priors=None, var_smo...          0.134108   \n",
       "20  {'classifier': LogisticRegression(C=100, class...          0.126716   \n",
       "30  {'classifier': GaussianNB(priors=None, var_smo...          0.128828   \n",
       "5   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.118268   \n",
       "6   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.127772   \n",
       "28  {'classifier': GaussianNB(priors=None, var_smo...          0.121436   \n",
       "13  {'classifier': LogisticRegression(C=100, class...          0.119324   \n",
       "3   {'classifier': LinearSVC(C=1.0, class_weight=N...          0.128828   \n",
       "31  {'classifier': GaussianNB(priors=None, var_smo...          0.125660   \n",
       "19  {'classifier': LogisticRegression(C=100, class...          0.129884   \n",
       "11  {'classifier': LogisticRegression(C=100, class...          0.129884   \n",
       "26  {'classifier': GaussianNB(priors=None, var_smo...          0.106653   \n",
       "21  {'classifier': LogisticRegression(C=100, class...          0.110876   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "23           0.135164           0.133192           0.136364   \n",
       "16           0.149947           0.131078           0.131078   \n",
       "8            0.149947           0.133192           0.125793   \n",
       "15           0.131996           0.135307           0.140592   \n",
       "10           0.138332           0.127907           0.132135   \n",
       "7            0.134108           0.121564           0.132135   \n",
       "18           0.138332           0.124736           0.133192   \n",
       "14           0.137276           0.119450           0.124736   \n",
       "0            0.144667           0.137421           0.118393   \n",
       "1            0.127772           0.131078           0.132135   \n",
       "2            0.140444           0.116279           0.136364   \n",
       "4            0.142555           0.117336           0.123679   \n",
       "22           0.131996           0.121564           0.133192   \n",
       "24           0.137276           0.125793           0.139535   \n",
       "25           0.142555           0.121564           0.122622   \n",
       "9            0.133052           0.124736           0.123679   \n",
       "17           0.131996           0.123679           0.124736   \n",
       "12           0.141499           0.113108           0.124736   \n",
       "29           0.128828           0.119450           0.125793   \n",
       "27           0.126716           0.119450           0.127907   \n",
       "20           0.136220           0.124736           0.122622   \n",
       "30           0.123548           0.112051           0.138478   \n",
       "5            0.141499           0.116279           0.133192   \n",
       "6            0.133052           0.117336           0.125793   \n",
       "28           0.121436           0.130021           0.138478   \n",
       "13           0.140444           0.114165           0.131078   \n",
       "3            0.125660           0.122622           0.124736   \n",
       "31           0.118268           0.116279           0.124736   \n",
       "19           0.125660           0.124736           0.118393   \n",
       "11           0.125660           0.124736           0.118393   \n",
       "26           0.127772           0.119450           0.123679   \n",
       "21           0.142555           0.113108           0.119450   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  Has Header + Footer  \\\n",
       "23           0.141649         0.137785        0.003679                Fasle   \n",
       "16           0.133192         0.136937        0.007183                Fasle   \n",
       "8            0.134249         0.136725        0.008084                Fasle   \n",
       "15           0.141649         0.136519        0.003921                Fasle   \n",
       "10           0.145877         0.134616        0.006713                Fasle   \n",
       "7            0.142706         0.134403        0.007607                Fasle   \n",
       "18           0.143763         0.133559        0.006905                Fasle   \n",
       "14           0.139535         0.132922        0.009224                Fasle   \n",
       "0            0.125793         0.132499        0.009274                Fasle   \n",
       "1            0.135307         0.132291        0.002802                Fasle   \n",
       "2            0.136364         0.132289        0.008439                Fasle   \n",
       "4            0.132135         0.131441        0.009843                Fasle   \n",
       "22           0.133192         0.130811        0.004671                Fasle   \n",
       "24           0.138478         0.130603        0.010564                Fasle   \n",
       "25           0.138478         0.129753        0.008904                Fasle   \n",
       "9            0.134249         0.129753        0.004562                Fasle   \n",
       "17           0.134249         0.129331        0.004277                Fasle   \n",
       "12           0.137421         0.129118        0.009978                Fasle   \n",
       "29           0.127907         0.129118        0.007951                Fasle   \n",
       "27           0.136364         0.128909        0.005965                Fasle   \n",
       "20           0.134249         0.128908        0.005361                Fasle   \n",
       "30           0.137421         0.128065        0.009730                Fasle   \n",
       "5            0.128964         0.127641        0.009398                Fasle   \n",
       "6            0.134249         0.127640        0.006043                Fasle   \n",
       "28           0.123679         0.127010        0.006539                Fasle   \n",
       "13           0.128964         0.126795        0.009216                Fasle   \n",
       "3            0.124736         0.125316        0.002019                Fasle   \n",
       "31           0.140592         0.125107        0.008543                Fasle   \n",
       "19           0.118393         0.123413        0.004451                Fasle   \n",
       "11           0.118393         0.123413        0.004451                Fasle   \n",
       "26           0.133192         0.122149        0.008981                Fasle   \n",
       "21           0.124736         0.122145        0.011307                Fasle   \n",
       "\n",
       "   rank_test_score  \n",
       "23               1  \n",
       "16               2  \n",
       "8                3  \n",
       "15               4  \n",
       "10               5  \n",
       "7                6  \n",
       "18               7  \n",
       "14               8  \n",
       "0                9  \n",
       "1               10  \n",
       "2               11  \n",
       "4               12  \n",
       "22              13  \n",
       "24              14  \n",
       "25              15  \n",
       "9               16  \n",
       "17              17  \n",
       "12              18  \n",
       "29              19  \n",
       "27              20  \n",
       "20              21  \n",
       "30              22  \n",
       "5               23  \n",
       "6               24  \n",
       "28              25  \n",
       "13              26  \n",
       "3               27  \n",
       "31              28  \n",
       "19              29  \n",
       "11              29  \n",
       "26              31  \n",
       "21              32  \n",
       "\n",
       "[32 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################\n",
    "## Grid Search Results : DATA with HEADERS + FOOTERS ##\n",
    "#######################################################\n",
    "# Add column to Table: Used data with Header and Footers REMOVED\n",
    "table_no_hf = pd.DataFrame(grid_no_hf.cv_results_)\n",
    "table_no_hf.insert (len(table_no_hf.columns)-1, 'Has Header + Footer', 'Fasle')\n",
    "\n",
    "# Print and order by best 'accuracy'\n",
    "table_no_hf.sort_values(by=['rank_test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_dim_reducer</th>\n",
       "      <th>param_dim_reducer__n_components</th>\n",
       "      <th>param_vectorizer__min_df</th>\n",
       "      <th>param_vectorizer__token_pattern</th>\n",
       "      <th>...</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>Has Header + Footer</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.378391</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.202152</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.142555</td>\n",
       "      <td>0.135164</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.141649</td>\n",
       "      <td>0.137785</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.408059</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.193447</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.863780</td>\n",
       "      <td>0.850053</td>\n",
       "      <td>0.852008</td>\n",
       "      <td>0.856237</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.670012</td>\n",
       "      <td>0.020358</td>\n",
       "      <td>0.161506</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.139388</td>\n",
       "      <td>0.149947</td>\n",
       "      <td>0.131078</td>\n",
       "      <td>0.131078</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.136937</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.484378</td>\n",
       "      <td>0.033207</td>\n",
       "      <td>0.201690</td>\n",
       "      <td>0.004794</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.865892</td>\n",
       "      <td>0.859556</td>\n",
       "      <td>0.841438</td>\n",
       "      <td>0.849894</td>\n",
       "      <td>0.864693</td>\n",
       "      <td>0.856295</td>\n",
       "      <td>0.009327</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.871167</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>0.201438</td>\n",
       "      <td>0.007013</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.859556</td>\n",
       "      <td>0.858501</td>\n",
       "      <td>0.842495</td>\n",
       "      <td>0.850951</td>\n",
       "      <td>0.867865</td>\n",
       "      <td>0.855874</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.264104</td>\n",
       "      <td>0.004610</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.710665</td>\n",
       "      <td>0.674762</td>\n",
       "      <td>0.683932</td>\n",
       "      <td>0.658562</td>\n",
       "      <td>0.690275</td>\n",
       "      <td>0.683639</td>\n",
       "      <td>0.017217</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.395729</td>\n",
       "      <td>0.019597</td>\n",
       "      <td>0.158933</td>\n",
       "      <td>0.007624</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.106653</td>\n",
       "      <td>0.127772</td>\n",
       "      <td>0.119450</td>\n",
       "      <td>0.123679</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.122149</td>\n",
       "      <td>0.008981</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.471277</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>0.287955</td>\n",
       "      <td>0.011295</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.699050</td>\n",
       "      <td>0.687434</td>\n",
       "      <td>0.668076</td>\n",
       "      <td>0.664905</td>\n",
       "      <td>0.688161</td>\n",
       "      <td>0.681525</td>\n",
       "      <td>0.012986</td>\n",
       "      <td>True</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.425403</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.215795</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': LogisticRegression(C=100, class...</td>\n",
       "      <td>0.110876</td>\n",
       "      <td>0.142555</td>\n",
       "      <td>0.113108</td>\n",
       "      <td>0.119450</td>\n",
       "      <td>0.124736</td>\n",
       "      <td>0.122145</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>Fasle</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.548457</td>\n",
       "      <td>0.010872</td>\n",
       "      <td>0.312255</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NMF(alpha=0.0, beta_loss='frobenius', init=Non...</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>(?u)\\b\\w\\w+\\b</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': GaussianNB(priors=None, var_smo...</td>\n",
       "      <td>0.684266</td>\n",
       "      <td>0.697994</td>\n",
       "      <td>0.653277</td>\n",
       "      <td>0.671247</td>\n",
       "      <td>0.677590</td>\n",
       "      <td>0.676875</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "23       0.378391      0.006370         0.202152        0.007484   \n",
       "10       1.408059      0.090800         0.193447        0.004733   \n",
       "16       0.670012      0.020358         0.161506        0.007492   \n",
       "8        1.484378      0.033207         0.201690        0.004794   \n",
       "16       0.871167      0.005852         0.201438        0.007013   \n",
       "..            ...           ...              ...             ...   \n",
       "31       0.320980      0.007667         0.264104        0.004610   \n",
       "26       0.395729      0.019597         0.158933        0.007624   \n",
       "30       0.471277      0.008275         0.287955        0.011295   \n",
       "21       0.425403      0.008214         0.215795        0.008208   \n",
       "28       0.548457      0.010872         0.312255        0.010124   \n",
       "\n",
       "                                     param_classifier param_classifier__C  \\\n",
       "23  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "10  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "16  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "8   LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "16  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "..                                                ...                 ...   \n",
       "31       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "26       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "30       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "21  LogisticRegression(C=100, class_weight=None, d...                 100   \n",
       "28       GaussianNB(priors=None, var_smoothing=1e-09)                 NaN   \n",
       "\n",
       "                                    param_dim_reducer  \\\n",
       "23  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "10  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "16  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "8   TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "16  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "..                                                ...   \n",
       "31  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "26  TruncatedSVD(algorithm='randomized', n_compone...   \n",
       "30  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "21  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "28  NMF(alpha=0.0, beta_loss='frobenius', init=Non...   \n",
       "\n",
       "   param_dim_reducer__n_components param_vectorizer__min_df  \\\n",
       "23                              50                        5   \n",
       "10                              50                        5   \n",
       "16                              50                        3   \n",
       "8                               50                        3   \n",
       "16                              50                        3   \n",
       "..                             ...                      ...   \n",
       "31                              50                        5   \n",
       "26                              50                        5   \n",
       "30                              50                        5   \n",
       "21                              50                        3   \n",
       "28                              50                        3   \n",
       "\n",
       "      param_vectorizer__token_pattern  ...  \\\n",
       "23  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "10                      (?u)\\b\\w\\w+\\b  ...   \n",
       "16                      (?u)\\b\\w\\w+\\b  ...   \n",
       "8                       (?u)\\b\\w\\w+\\b  ...   \n",
       "16                      (?u)\\b\\w\\w+\\b  ...   \n",
       "..                                ...  ...   \n",
       "31  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "26                      (?u)\\b\\w\\w+\\b  ...   \n",
       "30                      (?u)\\b\\w\\w+\\b  ...   \n",
       "21  (?u)\\b[^\\W\\d_][^\\W\\d_][^\\W\\d_]+\\b  ...   \n",
       "28                      (?u)\\b\\w\\w+\\b  ...   \n",
       "\n",
       "                                               params split0_test_score  \\\n",
       "23  {'classifier': LogisticRegression(C=100, class...          0.142555   \n",
       "10  {'classifier': LogisticRegression(C=100, class...          0.863780   \n",
       "16  {'classifier': LogisticRegression(C=100, class...          0.139388   \n",
       "8   {'classifier': LogisticRegression(C=100, class...          0.865892   \n",
       "16  {'classifier': LogisticRegression(C=100, class...          0.859556   \n",
       "..                                                ...               ...   \n",
       "31  {'classifier': GaussianNB(priors=None, var_smo...          0.710665   \n",
       "26  {'classifier': GaussianNB(priors=None, var_smo...          0.106653   \n",
       "30  {'classifier': GaussianNB(priors=None, var_smo...          0.699050   \n",
       "21  {'classifier': LogisticRegression(C=100, class...          0.110876   \n",
       "28  {'classifier': GaussianNB(priors=None, var_smo...          0.684266   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "23           0.135164           0.133192           0.136364   \n",
       "10           0.850053           0.852008           0.856237   \n",
       "16           0.149947           0.131078           0.131078   \n",
       "8            0.859556           0.841438           0.849894   \n",
       "16           0.858501           0.842495           0.850951   \n",
       "..                ...                ...                ...   \n",
       "31           0.674762           0.683932           0.658562   \n",
       "26           0.127772           0.119450           0.123679   \n",
       "30           0.687434           0.668076           0.664905   \n",
       "21           0.142555           0.113108           0.119450   \n",
       "28           0.697994           0.653277           0.671247   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  Has Header + Footer  \\\n",
       "23           0.141649         0.137785        0.003679                Fasle   \n",
       "10           0.863636         0.857143        0.005721                 True   \n",
       "16           0.133192         0.136937        0.007183                Fasle   \n",
       "8            0.864693         0.856295        0.009327                 True   \n",
       "16           0.867865         0.855874        0.008573                 True   \n",
       "..                ...              ...             ...                  ...   \n",
       "31           0.690275         0.683639        0.017217                 True   \n",
       "26           0.133192         0.122149        0.008981                Fasle   \n",
       "30           0.688161         0.681525        0.012986                 True   \n",
       "21           0.124736         0.122145        0.011307                Fasle   \n",
       "28           0.677590         0.676875        0.014763                 True   \n",
       "\n",
       "   rank_test_score  \n",
       "23               1  \n",
       "10               1  \n",
       "16               2  \n",
       "8                2  \n",
       "16               3  \n",
       "..             ...  \n",
       "31              30  \n",
       "26              31  \n",
       "30              31  \n",
       "21              32  \n",
       "28              32  \n",
       "\n",
       "[64 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################################################\n",
    "## Combine Findings of both Tables (W/wo) HEADERS + FOOTERS IN DATASET ##\n",
    "#########################################################################\n",
    "\n",
    "# Combine Both Tables\n",
    "combined_table = pd.concat([table_no_hf, table_hf])\n",
    "combined_table.sort_values(by=['rank_test_score'])\n",
    "\n",
    "## Note: Still some weirdness when combining tables. Overlapping indexes for ranking\n",
    "## Using data without headers/foots results in incredibly low scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
